# news_collector.py
# ë°±ì—”ë“œìš© ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ
# ìŠ¤íŠ¸ë¦¼ë¦¿ main_app.pyì˜ ìˆ˜ì§‘ ë¡œì§ì„ ë°±ì—”ë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¬êµ¬ì„±

from __future__ import annotations
import os, sys, json, sqlite3, time
from typing import List, Dict, Tuple, Optional, Iterable, Set
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import numpy as np
import pandas as pd
import requests
import feedparser
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# í™˜ê²½ì„¤ì • ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ íŒŒì‹± ìœ í‹¸
def _strip_comment(v: str) -> str:
    v = (v or "").split('#', 1)[0].strip()
    if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
        v = v[1:-1].strip()
    return v

def getenv_int(name: str, default: int) -> int:
    raw = _strip_comment(os.getenv(name, str(default)))
    m = re.search(r'-?\d+', raw)
    return int(m.group()) if m else int(default)

def getenv_float(name: str, default: float) -> float:
    raw = _strip_comment(os.getenv(name, str(default)))
    m = re.search(r'-?\d+(?:\.\d+)?', raw)
    return float(m.group()) if m else float(default)

def getenv_bool(name: str, default: bool=False) -> bool:
    raw = _strip_comment(os.getenv(name, str(default))).lower()
    return raw in ("1", "true", "t", "yes", "y", "on")

def getenv_str(name: str, default: str="") -> str:
    return _strip_comment(os.getenv(name, default))

# ì„¤ì •ê°’
MAX_RESULTS            = getenv_int("MAX_RESULTS", 10)
MAX_TOTAL_PER_SOURCE   = getenv_int("MAX_TOTAL_PER_SOURCE", 200)
RSS_BACKFILL_PAGES     = getenv_int("RSS_BACKFILL_PAGES", 3)

CONNECT_TIMEOUT        = getenv_float("CONNECT_TIMEOUT", 6.0)
READ_TIMEOUT           = getenv_float("READ_TIMEOUT", 10.0)
OPENAI_TIMEOUT         = getenv_float("OPENAI_TIMEOUT", 20.0)

ENABLE_SUMMARY         = getenv_bool("ENABLE_SUMMARY", False)
ENABLE_HTTP_CACHE      = getenv_bool("ENABLE_HTTP_CACHE", True)
HTTP_CACHE_EXPIRE      = getenv_int("HTTP_CACHE_EXPIRE", 3600)
PARALLEL_MAX_WORKERS   = getenv_int("PARALLEL_MAX_WORKERS", 8)
SKIP_UPDATE_IF_EXISTS  = getenv_bool("SKIP_UPDATE_IF_EXISTS", True)

NLP_BACKEND            = getenv_str("NLP_BACKEND", "kiwi").lower()
PER_ARTICLE_SLEEP      = getenv_float("PER_ARTICLE_SLEEP", 0.0)

STRICT_TECH_KEYWORDS   = getenv_bool("STRICT_TECH_KEYWORDS", True)
SKIP_NON_TECH          = getenv_bool("SKIP_NON_TECH", False)

OPENAI_API_KEY         = getenv_str("OPENAI_API_KEY", "")

# RSS í”¼ë“œ ì†ŒìŠ¤
FEEDS: List[Dict[str, str]] = [
    # --- Korea (ko) ---
    {"feed_url": "https://it.donga.com/feeds/rss/",            "source": "ITë™ì•„",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section902.xml",      "source": "ì „ìì‹ ë¬¸_ì†ë³´",         "category": "IT",           "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section901.xml",      "source": "ì „ìì‹ ë¬¸_ì˜¤ëŠ˜ì˜ë‰´ìŠ¤",     "category": "IT",           "lang": "ko"},
    {"feed_url": "https://zdnet.co.kr/news/news_xml.asp",      "source": "ZDNet Korea",         "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.itworld.co.kr/rss/all.xml",      "source": "ITWorld Korea",       "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.ciokorea.com/rss/all.xml",       "source": "CIO Korea",           "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.bloter.net/feed",                "source": "Bloter",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://byline.network/feed/",               "source": "Byline Network",      "category": "IT",           "lang": "ko"},
    {"feed_url": "https://platum.kr/feed",                     "source": "Platum",              "category": "Startup",      "lang": "ko"},
    {"feed_url": "https://www.boannews.com/media/news_rss.xml","source": "ë³´ì•ˆë‰´ìŠ¤",             "category": "Security",     "lang": "ko"},
    {"feed_url": "https://it.chosun.com/rss.xml",              "source": "ITì¡°ì„ ",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.ddaily.co.kr/news_rss.php",      "source": "ë””ì§€í„¸ë°ì¼ë¦¬",           "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.kbench.com/rss.xml",             "source": "KBench",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.sedaily.com/rss/IT.xml",         "source": "ì„œìš¸ê²½ì œ IT",           "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.hankyung.com/feed/it",           "source": "í•œêµ­ê²½ì œ IT",            "category": "IT",           "lang": "ko"},

    # --- Global (en) ---
    {"feed_url": "https://techcrunch.com/feed/",               "source": "TechCrunch",          "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.eetimes.com/feed/",              "source": "EE Times",            "category": "Electronics",  "lang": "en"},
    {"feed_url": "https://spectrum.ieee.org/rss/fulltext",     "source": "IEEE Spectrum",       "category": "Engineering",  "lang": "en"},
    {"feed_url": "http://export.arxiv.org/rss/cs",             "source": "arXiv CS",            "category": "Research",     "lang": "en"},
    {"feed_url": "https://www.nature.com/nel/atom.xml",        "source": "Nature Electronics",  "category": "Research",     "lang": "en"},
    {"feed_url": "https://www.technologyreview.com/feed/",     "source": "MIT Tech Review",     "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.theverge.com/rss/index.xml",     "source": "The Verge",           "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.wired.com/feed/rss",             "source": "WIRED",               "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.engadget.com/rss.xml",           "source": "Engadget",            "category": "Tech",         "lang": "en"},
    {"feed_url": "https://venturebeat.com/category/ai/feed/",  "source": "VentureBeat AI",      "category": "AI",           "lang": "en"},
]

HEADERS = {"User-Agent": "Mozilla/5.0 (NewsAgent/1.0)"}

# HTTP ì„¸ì…˜ ì„¤ì •
try:
    if ENABLE_HTTP_CACHE:
        from requests_cache import CachedSession
        SESSION = CachedSession('http_cache', expire_after=HTTP_CACHE_EXPIRE)
    else:
        SESSION = requests.Session()
except Exception:
    SESSION = requests.Session()

ADAPTER = requests.adapters.HTTPAdapter(pool_connections=10, pool_maxsize=20, max_retries=1)
SESSION.mount("http://", ADAPTER)
SESSION.mount("https://", ADAPTER)

# ë¶ˆìš©ì–´ ë° ê¸°ìˆ  í‚¤ì›Œë“œ ì •ì˜
STOP_EXACT: Set[str] = set(map(str.lower, """
ìˆ ìˆ˜ ê¹€ ê¸¸ ê°€ ë§ d ì–¼ b ë°± ë³´ ìœ„ ë…„ ëª… ë°”ê¾¸ ë§Œ ê²ƒ jtbc x í•˜ê¸° ì‘ ë” ëŠ” ì€ ì´ ê°€ ë¥¼ ì— ì™€ ê³¼ ë„ ìœ¼ë¡œ ë¡œ ë¶€í„° ì—ì„œ ê¹Œì§€ ì—ê²Œ í•œ ì™€/ê³¼ ì—ê²Œì„œ í•˜ í•˜ë‹¤ ì…ë‹ˆë‹¤ ê¸°ì ì‚¬ì§„ ì œê³µ ì˜ìƒ ê¸°ì‚¬ ì…ë ¥ ì „ ë‚  ì£¼ ì›” ë…„ ì˜¤ëŠ˜ ë‚´ì¼ ì–´ì œ
""".split()))

STOP_WORDS = set([
    "ê¸°ì","ë‰´ìŠ¤","íŠ¹íŒŒì›","ì˜¤ëŠ˜","ë§¤ìš°","ê¸°ì‚¬","ì‚¬ì§„","ì˜ìƒ","ì œê³µ","ì…ë ¥",
    "ê²ƒ","ìˆ˜","ë“±","ë°","ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ì§€ë‚œ","ì´ë²ˆ","ê´€ë ¨","ëŒ€í•œ","í†µí•´","ëŒ€í•´","ìœ„í•´",
    "ì…ë‹ˆë‹¤","í•œë‹¤","í–ˆë‹¤","í•˜ì˜€ë‹¤","ì—ì„œëŠ”","ì—ì„œ","ëŒ€í•œ","ì´ë‚ ","ë¼ë©°","ë‹¤ê³ ","ì˜€ë‹¤","í–ˆë‹¤ê°€","í•˜ë©°",
]) | STOP_EXACT

TECH_ALLOW_TERMS = set(map(str.lower, """
ai ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹ ìƒì„±í˜• ì±—gpt ë¡œë³´í‹±ìŠ¤ ë¡œë´‡ ìë™í™” í˜‘ë™ë¡œë´‡
ë°˜ë„ì²´ ë©”ëª¨ë¦¬ dram nand ddr sram hbm ì‹œìŠ¤í…œ ë°˜ë„ì²´ íŒŒìš´ë“œë¦¬ ì›¨ì´í¼ ì†Œì ê³µì • ë…¸ê´‘ euv ì¥ë¹„ ì†Œì¬
npu tpu gpu cpu dsp isp fpga asic ì¹©ì…‹ ì¹© ì„¤ê³„ ë¦¬ì†Œê·¸ë˜í”¼ íŒ¨í‚¤ì§• í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë”©
ì´ì°¨ì „ì§€ ë°°í„°ë¦¬ ess ì–‘ê·¹ì¬ ìŒê·¹ì¬ ì „í•´ì§ˆ ë¶„ë¦¬ë§‰ ê³ ì²´ì „ì§€ ì „ê³ ì²´ ì „ê¸°ì°¨ ev hev phev bms
ììœ¨ì£¼í–‰ ë¼ì´ë‹¤ ë ˆì´ë” ì„¼ì„œ ì¹´ë©”ë¼ ì œì–´ê¸° ecu v2x
í†µì‹  ë„¤íŠ¸ì›Œí¬ 5g 6g lte nr ìœ„ì„± mmwave ë°±í™€ fronthaul ìŠ¤ëª°ì…€
ict í´ë¼ìš°ë“œ ì—£ì§€ì»´í“¨íŒ… ì—£ì§€ ì»´í“¨íŒ… ì„œë²„ ë°ì´í„°ì„¼í„° ì¿ ë²„ë„¤í‹°ìŠ¤ ì»¨í…Œì´ë„ˆ devops cicd ì˜¤ë¸Œì íŠ¸ìŠ¤í† ë¦¬ì§€ ê°ì²´ì €ì¥
ì†Œí”„íŠ¸ì›¨ì–´ í”Œë«í¼ saas paas iaas ë³´ì•ˆ ì•”í˜¸ ì¸ì¦ í‚¤ê´€ë¦¬ í‚¤ì²´ì¸ ì·¨ì•½ì  ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸
í•€í…Œí¬ ë¸”ë¡ì²´ì¸ ë¶„ì‚°ì›ì¥ defi nft
ëª¨ë¸ í•™ìŠµ íŒŒì¸íŠœë‹ íŠœë‹ í”„ë¡¬í”„íŠ¸ ì¶”ë¡  ì¸í¼ëŸ°ìŠ¤ í† í° ì„ë² ë”© ê²½ëŸ‰í™” ì–‘ìí™” distillation ì§€ì‹ì¦ë¥˜
ì‚¬ë¬¼ì¸í„°ë„· iot ì‚°ì—…ìš©iot iiot plc scada mes erp
ë””ìŠ¤í”Œë ˆì´ oled qd ë§ˆì´í¬ë¡œ led lcd microled micro-led
ë°”ì´ì˜¤ ë°”ì´ì˜¤ì„¼ì„œ ìœ ì „ìì¹˜ë£Œì œ ì„¸í¬ì¹˜ë£Œì œ ì˜ë£Œê¸°ê¸° í—¬ìŠ¤ì¼€ì–´ ë””ì§€í„¸ í—¬ìŠ¤ ì›¨ì–´ëŸ¬ë¸” ì›ê²©ì§„ë£Œ
""".split()))

# Import database module
try:
    from database import db, get_db_connection, init_db
    DB_MODULE_AVAILABLE = True
except ImportError:
    DB_MODULE_AVAILABLE = False
    # Fallback to sqlite for development
    DB_PATH = getenv_str("DB_PATH", "news.db")
    
    def init_db():
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        cursor = conn.cursor()
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            link TEXT UNIQUE,
            published TEXT,
            source TEXT,
            raw_text TEXT,
            summary TEXT,
            keywords TEXT,
            category TEXT,
            created_at TEXT DEFAULT (datetime('now'))
        );
        """)
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS favorites (
            article_id INTEGER UNIQUE,
            created_at TEXT DEFAULT (datetime('now'))
        );
        """)
        conn.commit()
        conn.close()
        
    def get_db_connection():
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        return conn

def canonicalize_link(url: str) -> str:
    try:
        u = urlparse(url)
        scheme = (u.scheme or "https").lower()
        netloc = (u.netloc or "").lower()
        path = (u.path or "").rstrip("/")
        drop = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content","utm_id","utm_name",
                "gclid","fbclid","igshid","spm","ref","ref_src","cmpid"}
        qs = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True) if k.lower() not in drop]
        query = urlencode(qs, doseq=True)
        return urlunparse((scheme, netloc, path, u.params, query, ""))
    except Exception:
        return url

def extract_main_text(url: str) -> str:
    try:
        r = SESSION.get(url, headers=HEADERS, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        art = soup.find("article")
        if art and art.get_text(strip=True):
            return art.get_text("\n", strip=True)
        candidates = soup.select(
            "div[id*='content'], div[class*='content'], "
            "div[id*='article'], div[class*='article'], "
            "section[id*='content'], section[class*='content'], "
            "div[id*='news'], div[class*='news']"
        )
        best = max((c for c in candidates), key=lambda c: len(c.get_text(strip=True)), default=None)
        if best and len(best.get_text(strip=True)) > 200:
            return best.get_text("\n", strip=True)
        md = soup.find("meta", attrs={"name":"description"}) or soup.find("meta", attrs={"property":"og:description"})
        if md and md.get("content"):
            return md["content"]
    except Exception:
        pass
    return ""

def parse_feed(feed_url: str):
    try:
        feed = feedparser.parse(feed_url)
        if not hasattr(feed, "entries"): return None
        return feed
    except Exception:
        return None

def expand_paged_feed_urls(feed_url: str, pages: int) -> List[str]:
    """WordPress ê³„ì—´ /feed/ ì¸ ê²½ìš° ?paged=2..N ë¡œ í™•ì¥í•´ ê³¼ê±° ê¸°ì‚¬ë„ ìˆ˜ì§‘"""
    urls = [feed_url]
    if re.search(r"/feed/?$", feed_url, re.IGNORECASE):
        for i in range(2, max(2, pages+1)):
            sep = "&" if "?" in feed_url else "?"
            urls.append(f"{feed_url}{sep}paged={i}")
    return urls

def is_meaningless_token(w: str) -> bool:
    if not w: return True
    s = w.strip()
    if not s: return True
    sl = s.lower()
    if len(sl) == 1: return True
    if re.fullmatch(r"[\W_]+", sl): return True
    if re.fullmatch(r"\d+", sl): return True
    if re.fullmatch(r"[\u1100-\u11FF\u3130-\u318F]", s): return True
    if sl in STOP_EXACT: return True
    return False

def is_tech_term(w: str) -> bool:
    if not w: return False
    s = w.strip()
    sl = s.lower()
    if sl in TECH_ALLOW_TERMS: return True
    if "ë°˜ë„ì²´" in s or "ììœ¨ì£¼í–‰" in s or "í´ë¼ìš°ë“œ" in s or "ëª¨ë¸" in s or "ì•Œê³ ë¦¬ì¦˜" in s:
        return True
    return False

def sanitize_summary(s: Optional[str]) -> str:
    if not s: return ""
    t = str(s).strip()
    t = re.sub(r"^\s*\[[^\]]*\]\s*", "", t)
    t = re.sub(r"(^|\s)ì œëª©\s*:\s*", r"\1", t)
    t = re.sub(r"(^|\s)ì²«\s*ë¬¸ì¥\s*:\s*", r"\1", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _clean_sentences(txt: str, limit_chars: int = 700) -> str:
    t = (txt or "").strip()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"(ê¸°ì|ì‚¬ì§„|ì˜ìƒ)\s*=", "", t)
    return t[:limit_chars]

def summarize_kor(title: str, source: str, published: str, text: str) -> str:
    """ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (OpenAI ì—†ì´)"""
    base = _clean_sentences(text or "", 1200)
    if not base or len(base) < 20:
        return sanitize_summary(f"{title}".strip())
    sents = re.split(r"(?<=[.?!ã€‚])\s+", base)
    head = " ".join(sents[:3]) if sents else base
    return sanitize_summary(head)

def extract_keywords_simple(text: str, top_k: int = 30):
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ì—†ì´)"""
    if not text: return []
    
    # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
    tokens = re.findall(r'[ê°€-í£]+|[A-Za-z]+|\d+', text.lower())
    
    # ë¶ˆìš©ì–´ ì œê±°
    tokens = [t for t in tokens if t not in STOP_WORDS and not is_meaningless_token(t)]
    
    # ê¸°ìˆ  ìš©ì–´ í•„í„°ë§ (STRICT_TECH_KEYWORDSê°€ Trueì¼ ë•Œ)
    if STRICT_TECH_KEYWORDS:
        tokens = [t for t in tokens if is_tech_term(t)]
    
    # ë¹ˆë„ ê³„ì‚°
    counts = Counter(tokens)
    
    return [w for w, _ in counts.most_common(top_k)]

def is_tech_doc(title: str, body: str, keywords: Iterable[str]) -> bool:
    text = f"{title or ''} {body or ''} {' '.join(keywords or [])}"
    for k in (keywords or []):
        if is_tech_term(k): return True
    for term in TECH_ALLOW_TERMS:
        if term in text.lower(): return True
    if re.search(r"(ì—°ì˜ˆ|ìŠ¤íƒ€|ì˜ˆëŠ¥|í—¬ìŠ¤|ê±´ê°•|ë¼ì´í”„|ë§›ì§‘|ì—¬í–‰|ë·°í‹°|ìš´ì„¸|ê²Œì„ì‡¼|eìŠ¤í¬ì¸ )", text):
        return False
    return False

def link_exists(link: str) -> bool:
    if DB_MODULE_AVAILABLE:
        results = db.execute_query("SELECT 1 FROM articles WHERE link=? LIMIT 1", (link,))
        return len(results) > 0
    else:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM articles WHERE link=? LIMIT 1", (link,))
        result = cursor.fetchone() is not None
        conn.close()
        return result

def upsert_article(title, link, published, source, raw_text, summary, keywords):
    link = canonicalize_link(link)
    summary = sanitize_summary(summary)
    keywords_json = json.dumps(keywords, ensure_ascii=False)
    
    if DB_MODULE_AVAILABLE:
        try:
            # Try insert first
            db.execute_update("""
                INSERT INTO articles (title, link, published, source, raw_text, summary, keywords, category)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (title, link, published, source, raw_text, summary, keywords_json, None))
            return "insert"
        except:
            # If insert fails, try update
            db.execute_update("""
                UPDATE articles
                   SET title=?, published=?, source=?,
                       raw_text=?, summary=?, keywords=?
                 WHERE link=?
            """, (title, published, source, raw_text, summary, keywords_json, link))
            return "update"
    else:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        try:
            cursor.execute("""
                INSERT INTO articles (title, link, published, source, raw_text, summary, keywords, category)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (title, link, published, source, raw_text, summary, keywords_json, None))
            conn.commit()
            result = "insert"
        except sqlite3.IntegrityError:
            cursor.execute("""
                UPDATE articles
                   SET title=?, published=?, source=?,
                       raw_text=?, summary=?, keywords=?
                 WHERE link=?
            """, (title, published, source, raw_text, summary, keywords_json, link))
            conn.commit()
            result = "update"
        finally:
            conn.close()
        return result

def _process_entry(entry, idx, total, source):
    title = getattr(entry, "title", "").strip()
    link  = canonicalize_link(getattr(entry, "link", "").strip())
    if not (title and link):
        return "skip", idx

    if SKIP_UPDATE_IF_EXISTS and link_exists(link):
        return "skip", idx

    published = getattr(entry, "published", "") or getattr(entry, "updated", "") or datetime.utcnow().strftime("%Y-%m-%d")
    raw_text  = extract_main_text(link) or getattr(entry, "summary", "") or ""
    summary   = summarize_kor(title, source, published, raw_text or title)
    keywords  = extract_keywords_simple(raw_text or summary, top_k=30)

    if SKIP_NON_TECH and not is_tech_doc(title, raw_text, keywords):
        return "skip_nontech", idx

    res = upsert_article(title, link, published, source, raw_text, summary, keywords)

    if PER_ARTICLE_SLEEP > 0:
        time.sleep(PER_ARTICLE_SLEEP)
    return res, idx

def fetch_and_store_news(feed_url: str, source: str, max_total=None):
    if max_total is None:
        max_total = MAX_TOTAL_PER_SOURCE

    if not feed_url:
        print(f"- {source}: feed_url ì—†ìŒ â†’ ê±´ë„ˆëœ€")
        return

    urls = expand_paged_feed_urls(feed_url, RSS_BACKFILL_PAGES)
    print(f"**â–· {source}** í”¼ë“œ ì½ëŠ” ì¤‘â€¦ (í™•ì¥ {len(urls)}ê°œ)")

    entries_all = []
    for i, u in enumerate(urls, 1):
        feed = parse_feed(u)
        if feed is None:
            print(f"  - RSS íŒŒì‹± ì‹¤íŒ¨/ë¹„í˜¸í™˜: {u} â†’ ê±´ë„ˆëœ€")
            continue
        entries = feed.entries or []
        if MAX_RESULTS:
            entries = entries[:MAX_RESULTS]
        entries_all.extend(entries)
        print(f"  - {i}/{len(urls)}: í•­ëª© {len(entries)}ê±´")

        if len(entries_all) >= max_total:
            break

    if not entries_all:
        print(f"â—¼ {source}: ìˆ˜ì§‘ëœ í•­ëª© ì—†ìŒ")
        return

    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    seen = set()
    uniq_entries = []
    for e in entries_all:
        link = canonicalize_link(getattr(e, "link", ""))
        if not link or link in seen:
            continue
        seen.add(link)
        uniq_entries.append(e)

    print(f"- ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(uniq_entries)}ê±´ (ì¤‘ë³µ ì œê±° í›„)")

    inserted, updated, skipped, skipped_nontech = 0, 0, 0, 0

    if not ENABLE_SUMMARY and len(uniq_entries) > 0:
        workers = max(1, PARALLEL_MAX_WORKERS)
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futs = [ex.submit(_process_entry, e, i, len(uniq_entries), source)
                    for i, e in enumerate(uniq_entries, 1)]
            for fut in as_completed(futs):
                res, idx = fut.result()
                if   res == "insert": inserted += 1; print(f"[{idx}/{len(uniq_entries)}] âœ… ì‹ ê·œ ì €ì¥")
                elif res == "update": updated  += 1; print(f"[{idx}/{len(uniq_entries)}] ğŸ”„ ì—…ë°ì´íŠ¸")
                elif res == "skip_nontech":    skipped_nontech += 1
                else:                           skipped += 1
    else:
        for idx, entry in enumerate(uniq_entries, 1):
            res, _ = _process_entry(entry, idx, len(uniq_entries), source)
            if   res == "insert": inserted += 1; print(f"[{idx}/{len(uniq_entries)}] âœ… ì‹ ê·œ ì €ì¥")
            elif res == "update": updated  += 1; print(f"[{idx}/{len(uniq_entries)}] ğŸ”„ ì—…ë°ì´íŠ¸")
            elif res == "skip_nontech":    skipped_nontech += 1
            else:                           skipped += 1

    print(f"â—¼ {source}: ì‹ ê·œ {inserted} Â· ì—…ë°ì´íŠ¸ {updated} Â· ìŠ¤í‚µ {skipped} Â· ë¹„ê¸°ìˆ ìŠ¤í‚µ {skipped_nontech}")

def collect_all_news():
    """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ ìˆ˜ì§‘"""
    print("â³ ë‰´ìŠ¤ ìˆ˜ì§‘/ìš”ì•½/í‚¤ì›Œë“œ ì‹œì‘")
    for f in FEEDS:
        fetch_and_store_news(f.get("feed_url"), f.get("source","(unknown)"))
    print("âœ… ëª¨ë“  ì†ŒìŠ¤ ì²˜ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    init_db()
    collect_all_news()