"""
Enhanced News Collector with advanced features
Integrates Streamlit functionality with backend API
"""

import os
import re
import json
import time
import logging
from typing import List, Dict, Optional, Set
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
import asyncio

import requests
import feedparser
from bs4 import BeautifulSoup

# Import database
from database import db

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Enhanced configuration
MAX_RESULTS = int(os.getenv("MAX_RESULTS", "15"))
MAX_TOTAL_PER_SOURCE = int(os.getenv("MAX_TOTAL_PER_SOURCE", "200"))
RSS_BACKFILL_PAGES = int(os.getenv("RSS_BACKFILL_PAGES", "3"))

CONNECT_TIMEOUT = float(os.getenv("CONNECT_TIMEOUT", "10.0"))
READ_TIMEOUT = float(os.getenv("READ_TIMEOUT", "15.0"))

ENABLE_SUMMARY = os.getenv("ENABLE_SUMMARY", "false").lower() == "true"
ENABLE_HTTP_CACHE = os.getenv("ENABLE_HTTP_CACHE", "true").lower() == "true"
HTTP_CACHE_EXPIRE = int(os.getenv("HTTP_CACHE_EXPIRE", "3600"))
PARALLEL_MAX_WORKERS = int(os.getenv("PARALLEL_MAX_WORKERS", "8"))
SKIP_UPDATE_IF_EXISTS = os.getenv("SKIP_UPDATE_IF_EXISTS", "true").lower() == "true"

STRICT_TECH_KEYWORDS = os.getenv("STRICT_TECH_KEYWORDS", "true").lower() == "true"
SKIP_NON_TECH = os.getenv("SKIP_NON_TECH", "false").lower() == "true"

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# Comprehensive RSS feeds (Korean + Global)
FEEDS = [
    # Korean Tech News
    {"feed_url": "https://it.donga.com/feeds/rss/", "source": "ITë™ì•„", "category": "IT", "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section902.xml", "source": "ì „ìžì‹ ë¬¸_ì†ë³´", "category": "IT", "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section901.xml", "source": "ì „ìžì‹ ë¬¸_ì˜¤ëŠ˜ì˜ë‰´ìŠ¤", "category": "IT", "lang": "ko"},
    {"feed_url": "https://zdnet.co.kr/news/news_xml.asp", "source": "ZDNet Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.itworld.co.kr/rss/all.xml", "source": "ITWorld Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.ciokorea.com/rss/all.xml", "source": "CIO Korea", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.bloter.net/feed", "source": "Bloter", "category": "IT", "lang": "ko"},
    {"feed_url": "https://byline.network/feed/", "source": "Byline Network", "category": "IT", "lang": "ko"},
    {"feed_url": "https://platum.kr/feed", "source": "Platum", "category": "Startup", "lang": "ko"},
    {"feed_url": "https://www.boannews.com/media/news_rss.xml", "source": "ë³´ì•ˆë‰´ìŠ¤", "category": "Security", "lang": "ko"},
    {"feed_url": "https://it.chosun.com/rss.xml", "source": "ITì¡°ì„ ", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.ddaily.co.kr/news_rss.php", "source": "ë””ì§€í„¸ë°ì¼ë¦¬", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.kbench.com/rss.xml", "source": "KBench", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.sedaily.com/rss/IT.xml", "source": "ì„œìš¸ê²½ì œ IT", "category": "IT", "lang": "ko"},
    {"feed_url": "https://www.hankyung.com/feed/it", "source": "í•œêµ­ê²½ì œ IT", "category": "IT", "lang": "ko"},
    
    # Global Tech News
    {"feed_url": "https://techcrunch.com/feed/", "source": "TechCrunch", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.eetimes.com/feed/", "source": "EE Times", "category": "Electronics", "lang": "en"},
    {"feed_url": "https://spectrum.ieee.org/rss/fulltext", "source": "IEEE Spectrum", "category": "Engineering", "lang": "en"},
    {"feed_url": "https://www.technologyreview.com/feed/", "source": "MIT Tech Review", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.theverge.com/rss/index.xml", "source": "The Verge", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.wired.com/feed/rss", "source": "WIRED", "category": "Tech", "lang": "en"},
    {"feed_url": "https://www.engadget.com/rss.xml", "source": "Engadget", "category": "Tech", "lang": "en"},
    {"feed_url": "https://venturebeat.com/category/ai/feed/", "source": "VentureBeat AI", "category": "AI", "lang": "en"},
    {"feed_url": "https://arstechnica.com/feed/", "source": "Ars Technica", "category": "Tech", "lang": "en"},
    {"feed_url": "https://feeds.feedburner.com/oreilly/radar", "source": "O'Reilly Radar", "category": "Tech", "lang": "en"},
]

# HTTP Session with caching
HEADERS = {"User-Agent": "Mozilla/5.0 (NewsAgent/2.0; +https://github.com/newsbot)"}

try:
    if ENABLE_HTTP_CACHE:
        from requests_cache import CachedSession
        SESSION = CachedSession('http_cache', expire_after=HTTP_CACHE_EXPIRE)
    else:
        SESSION = requests.Session()
except ImportError:
    SESSION = requests.Session()
    logger.warning("requests-cache not available, using regular session")

# Configure HTTP adapter with retry strategy
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

retry_strategy = Retry(
    total=3,
    backoff_factor=0.3,
    status_forcelist=[429, 500, 502, 503, 504],
)
ADAPTER = HTTPAdapter(
    pool_connections=10,
    pool_maxsize=20,
    max_retries=retry_strategy
)
SESSION.mount("http://", ADAPTER)
SESSION.mount("https://", ADAPTER)

# Enhanced keyword processing
STOP_WORDS = {
    "ê¸°ìž", "ë‰´ìŠ¤", "íŠ¹íŒŒì›", "ì˜¤ëŠ˜", "ë§¤ìš°", "ê¸°ì‚¬", "ì‚¬ì§„", "ì˜ìƒ", "ì œê³µ", "ìž…ë ¥",
    "ê²ƒ", "ìˆ˜", "ë“±", "ë°", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ì§€ë‚œ", "ì´ë²ˆ", "ê´€ë ¨", "ëŒ€í•œ", "í†µí•´", "ëŒ€í•´", "ìœ„í•´",
    "ìž…ë‹ˆë‹¤", "í•œë‹¤", "í–ˆë‹¤", "í•˜ì˜€ë‹¤", "ì—ì„œëŠ”", "ì—ì„œ", "ëŒ€í•œ", "ì´ë‚ ", "ë¼ë©°", "ë‹¤ê³ ", "ì˜€ë‹¤", "í–ˆë‹¤ê°€", "í•˜ë©°",
    "the", "and", "for", "are", "but", "not", "you", "all", "can", "had", "her", "was", "one", "our"
}

TECH_KEYWORDS = {
    # AI/ML
    "ai", "ì¸ê³µì§€ëŠ¥", "machine learning", "ë¨¸ì‹ ëŸ¬ë‹", "deep learning", "ë”¥ëŸ¬ë‹",
    "chatgpt", "gpt", "llm", "ìƒì„±í˜•ai", "generative ai", "ì‹ ê²½ë§", "neural network",
    
    # Hardware/Semiconductors
    "ë°˜ë„ì²´", "semiconductor", "ë©”ëª¨ë¦¬", "memory", "dram", "nand", "hbm",
    "gpu", "cpu", "npu", "tpu", "fpga", "asic", "ì¹©ì…‹", "chipset",
    "ì‚¼ì„±ì „ìž", "samsung", "skí•˜ì´ë‹‰ìŠ¤", "tsmc", "ì—”ë¹„ë””ì•„", "nvidia",
    
    # Communication/Network
    "5g", "6g", "lte", "ì™€ì´íŒŒì´", "wifi", "ë¸”ë£¨íˆ¬ìŠ¤", "bluetooth",
    "í´ë¼ìš°ë“œ", "cloud", "ë°ì´í„°ì„¼í„°", "data center", "ì„œë²„", "server",
    "ë„¤íŠ¸ì›Œí¬", "network", "cdn", "api", "sdk",
    
    # Blockchain/Fintech
    "ë¸”ë¡ì²´ì¸", "blockchain", "ì•”í˜¸í™”í", "cryptocurrency", "bitcoin", "ë¹„íŠ¸ì½”ì¸",
    "ethereum", "ì´ë”ë¦¬ì›€", "nft", "defi", "ë©”íƒ€ë²„ìŠ¤", "metaverse",
    
    # Automotive/Energy
    "ìžìœ¨ì£¼í–‰", "autonomous", "ì „ê¸°ì°¨", "electric vehicle", "ev", "tesla", "í…ŒìŠ¬ë¼",
    "ë°°í„°ë¦¬", "battery", "ë¦¬íŠ¬", "lithium", "ìˆ˜ì†Œ", "hydrogen",
    
    # Security
    "ë³´ì•ˆ", "security", "í•´í‚¹", "hacking", "ì‚¬ì´ë²„", "cyber", "ëžœì„¬ì›¨ì–´", "ransomware",
    "ê°œì¸ì •ë³´", "privacy", "ë°ì´í„°ë³´í˜¸", "gdpr", "ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸", "zero trust",
    
    # Software/Development
    "ì˜¤í”ˆì†ŒìŠ¤", "open source", "ê°œë°œìž", "developer", "í”„ë¡œê·¸ëž˜ë°", "programming",
    "python", "javascript", "react", "node.js", "docker", "kubernetes",
}

class EnhancedNewsCollector:
    def __init__(self):
        self.session = SESSION
        self.stats = {
            'total_processed': 0,
            'total_inserted': 0,
            'total_updated': 0,
            'total_skipped': 0,
            'failed_feeds': [],
            'successful_feeds': []
        }
    
    def canonicalize_link(self, url: str) -> str:
        """Normalize and clean URL"""
        try:
            u = urlparse(url)
            scheme = (u.scheme or "https").lower()
            netloc = (u.netloc or "").lower()
            path = (u.path or "").rstrip("/")
            
            # Remove tracking parameters
            drop_params = {
                "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content", "utm_id",
                "gclid", "fbclid", "igshid", "spm", "ref", "ref_src", "cmpid", "source"
            }
            qs = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True) 
                  if k.lower() not in drop_params]
            query = urlencode(qs, doseq=True)
            
            return urlunparse((scheme, netloc, path, u.params, query, ""))
        except Exception as e:
            logger.warning(f"URL canonicalization failed for {url}: {e}")
            return url
    
    def extract_main_text(self, url: str) -> str:
        """Extract main content from article URL"""
        try:
            response = self.session.get(
                url, 
                headers=HEADERS, 
                timeout=(CONNECT_TIMEOUT, READ_TIMEOUT),
                allow_redirects=True
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # Remove unwanted elements
            for element in soup(["script", "style", "nav", "footer", "aside", "advertisement"]):
                element.decompose()
            
            # Try to find main content
            content_selectors = [
                "article",
                "[id*='content']", "[class*='content']",
                "[id*='article']", "[class*='article']", 
                "[class*='post-content']", "[class*='entry-content']",
                "[id*='story']", "[class*='story']",
                "main", ".main-content"
            ]
            
            best_content = ""
            max_length = 0
            
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(separator="\n", strip=True)
                    if len(text) > max_length and len(text) > 200:
                        max_length = len(text)
                        best_content = text
            
            # Fallback to meta description
            if not best_content or len(best_content) < 100:
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if not meta_desc:
                    meta_desc = soup.find("meta", attrs={"property": "og:description"})
                if meta_desc and meta_desc.get("content"):
                    best_content = meta_desc["content"]
            
            return best_content[:3000]  # Limit length
            
        except Exception as e:
            logger.warning(f"Failed to extract text from {url}: {e}")
            return ""
    
    def extract_keywords(self, text: str, title: str = "", top_k: int = 15) -> List[str]:
        """Enhanced keyword extraction"""
        if not text and not title:
            return []
        
        combined_text = f"{title} {text}".lower()
        keywords = []
        
        # 1. Extract tech keywords
        for keyword in TECH_KEYWORDS:
            if keyword in combined_text:
                keywords.append(keyword.title() if keyword.islower() else keyword)
        
        # 2. Extract patterns
        patterns = [
            r'\b[A-Z]{2,5}\b',  # Acronyms
            r'\b\d+[A-Za-z]{1,3}\b',  # Version numbers
            r'[ê°€-íž£]{2,8}(?:ê¸°ìˆ |ì‹œìŠ¤í…œ|í”Œëž«í¼|ì„œë¹„ìŠ¤|ì†”ë£¨ì…˜)',  # Korean tech terms
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text + " " + title)
            for match in matches:
                if len(match) >= 2 and match.lower() not in STOP_WORDS:
                    keywords.append(match)
        
        # 3. Remove duplicates and filter
        unique_keywords = []
        seen = set()
        for kw in keywords:
            kw_lower = kw.lower()
            if kw_lower not in seen and kw_lower not in STOP_WORDS and len(kw) >= 2:
                unique_keywords.append(kw)
                seen.add(kw_lower)
        
        return unique_keywords[:top_k]
    
    def summarize_text(self, title: str, text: str, source: str) -> str:
        """Generate summary (with optional OpenAI integration)"""
        if ENABLE_SUMMARY and OPENAI_API_KEY:
            return self._openai_summarize(title, text, source)
        else:
            return self._heuristic_summarize(title, text)
    
    def _openai_summarize(self, title: str, text: str, source: str) -> str:
        """OpenAI-based summarization"""
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            
            prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ë¥¼ 3-4ë¬¸ìž¥ìœ¼ë¡œ í•œêµ­ì–´ ìš”ì•½í•˜ì„¸ìš”. ì‚¬ì‹¤ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ.
- ì œëª©: {title}
- ì¶œì²˜: {source}
- ë³¸ë¬¸: {text[:2000]}

í•µì‹¬ ê¸°ìˆ /ì œí’ˆ/ìˆ˜ì¹˜/ì¼ì •ì„ í¬í•¨í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”.
"""
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ IT/ê¸°ìˆ  ë‰´ìŠ¤ ì „ë¬¸ ì—ë””í„°ìž…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.3
            )
            
            summary = response.choices[0].message.content.strip()
            return summary if len(summary) > 20 else self._heuristic_summarize(title, text)
            
        except Exception as e:
            logger.warning(f"OpenAI summarization failed: {e}")
            return self._heuristic_summarize(title, text)
    
    def _heuristic_summarize(self, title: str, text: str) -> str:
        """Rule-based summarization"""
        if not text:
            return title
        
        # Clean and split into sentences
        clean_text = re.sub(r'\s+', ' ', text)
        sentences = re.split(r'(?<=[.!?])\s+', clean_text)
        
        # Take first 2-3 sentences
        summary_sentences = []
        char_count = 0
        for sentence in sentences:
            if char_count + len(sentence) > 300:
                break
            summary_sentences.append(sentence.strip())
            char_count += len(sentence)
            if len(summary_sentences) >= 3:
                break
        
        summary = " ".join(summary_sentences)
        return summary if summary else title
    
    def is_tech_article(self, title: str, text: str, keywords: List[str]) -> bool:
        """Determine if article is tech-related"""
        if not STRICT_TECH_KEYWORDS:
            return True
        
        combined = f"{title} {text} {' '.join(keywords)}".lower()
        
        # Check for tech keywords
        tech_score = sum(1 for kw in TECH_KEYWORDS if kw in combined)
        
        # Check for non-tech indicators
        non_tech_patterns = [
            "ì—°ì˜ˆ", "ìŠ¤í¬ì¸ ", "ì˜ˆëŠ¥", "ë“œë¼ë§ˆ", "ì˜í™”", "ìŒì•…", "ê²Œìž„",
            "ìš”ë¦¬", "ì—¬í–‰", "íŒ¨ì…˜", "ë·°í‹°", "ê±´ê°•", "ìš´ë™", "ë‹¤ì´ì–´íŠ¸"
        ]
        non_tech_score = sum(1 for pattern in non_tech_patterns if pattern in combined)
        
        return tech_score > non_tech_score and tech_score > 0
    
    def parse_feed(self, feed_url: str) -> Optional[object]:
        """Parse RSS feed with error handling"""
        try:
            feed = feedparser.parse(feed_url)
            if hasattr(feed, "entries") and feed.entries:
                return feed
            return None
        except Exception as e:
            logger.error(f"RSS parsing failed for {feed_url}: {e}")
            return None
    
    def expand_paged_feed_urls(self, feed_url: str, pages: int = RSS_BACKFILL_PAGES) -> List[str]:
        """Expand WordPress-style feeds with pagination"""
        urls = [feed_url]
        if re.search(r"/feed/?$", feed_url, re.IGNORECASE):
            for i in range(2, pages + 1):
                sep = "&" if "?" in feed_url else "?"
                urls.append(f"{feed_url}{sep}paged={i}")
        return urls
    
    def process_entry(self, entry, source: str, category: str, language: str) -> Optional[Dict]:
        """Process individual RSS entry"""
        try:
            title = getattr(entry, "title", "").strip()
            link = self.canonicalize_link(getattr(entry, "link", "").strip())
            
            if not title or not link:
                return None
            
            # Check if already exists (if skip option is enabled)
            if SKIP_UPDATE_IF_EXISTS:
                existing = db.execute_query(
                    "SELECT id FROM articles WHERE link = %s" if db.db_type == "postgresql" else 
                    "SELECT id FROM articles WHERE link = ?", 
                    (link,)
                )
                if existing:
                    return None
            
            published = getattr(entry, "published", "") or getattr(entry, "updated", "")
            if not published:
                published = datetime.now().isoformat()
            else:
                try:
                    # Parse and normalize date
                    import dateutil.parser
                    parsed_date = dateutil.parser.parse(published)
                    published = parsed_date.isoformat()
                except:
                    published = datetime.now().isoformat()
            
            # Extract content
            raw_text = self.extract_main_text(link)
            if not raw_text:
                raw_text = getattr(entry, "summary", "") or getattr(entry, "description", "")
            
            # Generate summary and keywords
            summary = self.summarize_text(title, raw_text, source)
            keywords = self.extract_keywords(raw_text, title)
            
            # Filter tech articles if enabled
            if SKIP_NON_TECH and not self.is_tech_article(title, raw_text, keywords):
                return None
            
            article_data = {
                'title': title,
                'link': link,
                'published': published,
                'source': source,
                'raw_text': raw_text,
                'summary': summary,
                'keywords': keywords,
                'category': category,
                'language': language
            }
            
            return article_data
            
        except Exception as e:
            logger.error(f"Error processing entry from {source}: {e}")
            return None
    
    def collect_from_feed(self, feed_config: Dict) -> List[Dict]:
        """Collect articles from single feed"""
        feed_url = feed_config.get("feed_url")
        source = feed_config.get("source", "Unknown")
        category = feed_config.get("category", "News")
        language = feed_config.get("lang", "en")
        
        if not feed_url:
            return []
        
        try:
            logger.info(f"ðŸ“¡ Collecting from {source}")
            
            # Expand URLs for pagination
            urls = self.expand_paged_feed_urls(feed_url)
            all_entries = []
            
            for url in urls[:3]:  # Limit pages
                feed = self.parse_feed(url)
                if feed and feed.entries:
                    entries = feed.entries[:MAX_RESULTS]
                    all_entries.extend(entries)
                    logger.info(f"  ðŸ“„ {len(entries)} entries from page")
                
                if len(all_entries) >= MAX_TOTAL_PER_SOURCE:
                    break
            
            if not all_entries:
                logger.warning(f"âŒ No entries found for {source}")
                self.stats['failed_feeds'].append(source)
                return []
            
            # Process entries in parallel
            articles = []
            if PARALLEL_MAX_WORKERS > 1:
                with ThreadPoolExecutor(max_workers=min(PARALLEL_MAX_WORKERS, 4)) as executor:
                    futures = [
                        executor.submit(self.process_entry, entry, source, category, language)
                        for entry in all_entries
                    ]
                    
                    for future in as_completed(futures):
                        try:
                            article = future.result()
                            if article:
                                articles.append(article)
                        except Exception as e:
                            logger.error(f"Error in parallel processing: {e}")
            else:
                # Sequential processing
                for entry in all_entries:
                    article = self.process_entry(entry, source, category, language)
                    if article:
                        articles.append(article)
            
            logger.info(f"âœ… {source}: {len(articles)} articles processed")
            self.stats['successful_feeds'].append(source)
            return articles
            
        except Exception as e:
            logger.error(f"âŒ Failed to collect from {source}: {e}")
            self.stats['failed_feeds'].append(source)
            return []
    
    def save_articles(self, articles: List[Dict]) -> Dict[str, int]:
        """Save articles to database"""
        if not articles:
            return {'inserted': 0, 'updated': 0, 'skipped': 0}
        
        stats = {'inserted': 0, 'updated': 0, 'skipped': 0}
        
        for article in articles:
            try:
                # Check if article exists
                existing = db.execute_query(
                    "SELECT id FROM articles WHERE link = %s" if db.db_type == "postgresql" else
                    "SELECT id FROM articles WHERE link = ?",
                    (article['link'],)
                )
                
                if existing:
                    # Update existing article
                    if db.db_type == "postgresql":
                        db.execute_update("""
                            UPDATE articles SET 
                                title = %s, summary = %s, keywords = %s, updated_at = CURRENT_TIMESTAMP
                            WHERE link = %s
                        """, (
                            article['title'],
                            article['summary'],
                            json.dumps(article['keywords']),
                            article['link']
                        ))
                    else:
                        db.execute_update("""
                            UPDATE articles SET 
                                title = ?, summary = ?, keywords = ?, updated_at = datetime('now')
                            WHERE link = ?
                        """, (
                            article['title'],
                            article['summary'],
                            json.dumps(article['keywords']),
                            article['link']
                        ))
                    stats['updated'] += 1
                else:
                    # Insert new article
                    article_id = db.insert_article(article)
                    if article_id:
                        stats['inserted'] += 1
                    else:
                        stats['skipped'] += 1
                        
            except Exception as e:
                logger.error(f"Error saving article: {e}")
                stats['skipped'] += 1
        
        return stats
    
    def collect_all_news(self, max_feeds: Optional[int] = None) -> Dict:
        """Collect news from all feeds"""
        logger.info("ðŸš€ Starting comprehensive news collection")
        
        # Reset stats
        self.stats = {
            'total_processed': 0,
            'total_inserted': 0,
            'total_updated': 0,
            'total_skipped': 0,
            'failed_feeds': [],
            'successful_feeds': []
        }
        
        start_time = time.time()
        feeds_to_process = FEEDS[:max_feeds] if max_feeds else FEEDS
        
        all_articles = []
        
        # Process feeds in parallel
        if PARALLEL_MAX_WORKERS > 1:
            with ThreadPoolExecutor(max_workers=min(PARALLEL_MAX_WORKERS, len(feeds_to_process))) as executor:
                future_to_feed = {
                    executor.submit(self.collect_from_feed, feed): feed 
                    for feed in feeds_to_process
                }
                
                for future in as_completed(future_to_feed):
                    try:
                        articles = future.result(timeout=60)  # 60 second timeout per feed
                        all_articles.extend(articles)
                    except Exception as e:
                        feed = future_to_feed[future]
                        logger.error(f"Feed collection failed: {feed.get('source', 'Unknown')}: {e}")
        else:
            # Sequential processing
            for feed in feeds_to_process:
                articles = self.collect_from_feed(feed)
                all_articles.extend(articles)
        
        # Remove duplicates based on link
        unique_articles = []
        seen_links = set()
        for article in all_articles:
            if article['link'] not in seen_links:
                unique_articles.append(article)
                seen_links.add(article['link'])
        
        logger.info(f"ðŸ“Š Collected {len(unique_articles)} unique articles")
        
        # Save to database
        if unique_articles:
            save_stats = self.save_articles(unique_articles)
            self.stats.update(save_stats)
            self.stats['total_processed'] = len(unique_articles)
        
        end_time = time.time()
        duration = end_time - start_time
        
        logger.info(f"âœ… Collection completed in {duration:.2f} seconds")
        logger.info(f"ðŸ“ˆ Stats: {self.stats}")
        
        return {
            'success': True,
            'duration': duration,
            'stats': self.stats,
            'total_feeds': len(feeds_to_process),
            'successful_feeds': len(self.stats['successful_feeds']),
            'failed_feeds': len(self.stats['failed_feeds'])
        }

# Global collector instance
collector = EnhancedNewsCollector()

def collect_news_sync(max_feeds: Optional[int] = None) -> Dict:
    """Synchronous news collection"""
    return collector.collect_all_news(max_feeds)

async def collect_news_async(max_feeds: Optional[int] = None) -> Dict:
    """Asynchronous news collection"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, collect_news_sync, max_feeds)