# app.1.py â€” ìˆ˜ì§‘(í¬ë¡¤ë§/ìš”ì•½/í‚¤ì›Œë“œ) + ì‹œê°í™” + ì¦ê²¨ì°¾ê¸° + ì‡ìŠˆí”½
# ì‹¤í–‰ ì˜ˆ: streamlit run "C:\Users\User\Desktop\app (2)\app\app.1.py"
#

## (ìµœì´ˆ 1íšŒ) ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì •ì±… í—ˆìš©
#Set-ExecutionPolicy -Scope CurrentUser RemoteSigned

# ì‹¤í–‰
#powershell -ExecutionPolicy Bypass -File "C:\Users\ê¶Œë¯¼ì„œ\OneDrive\ë°”íƒ• í™”ë©´\app\update_data.ps1"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# .env ì˜ˆì‹œ(ì•± íŒŒì¼ê³¼ ê°™ì€ í´ë”ì— .env ìƒì„±)
# MAX_RESULTS=10
# MAX_TOTAL_PER_SOURCE=200          # ì†ŒìŠ¤ë‹¹ ì´ ìµœëŒ€ ì²˜ë¦¬ ê¸°ì‚¬ ìˆ˜(ë°±í•„ í¬í•¨)
# RSS_BACKFILL_PAGES=3              # WordPress ê³„ì—´ í”¼ë“œ: ?paged=2..N ìœ¼ë¡œ ê³¼ê±° ìˆ˜ì§‘
# ENABLE_SUMMARY=0
# ENABLE_HTTP_CACHE=1
# HTTP_CACHE_EXPIRE=3600
# PARALLEL_MAX_WORKERS=8
# SKIP_UPDATE_IF_EXISTS=1
# UI_LOAD_LIMIT=2000
# CONNECT_TIMEOUT=6
# READ_TIMEOUT=10
# OPENAI_TIMEOUT=20
# NLP_BACKEND=kiwi
# PER_ARTICLE_SLEEP=0
# OPENAI_API_KEY=sk-...          # ENABLE_SUMMARY=1ì¼ ë•Œë§Œ í•„ìš”
# ENABLE_GITHUB=0
# GITHUB_TOKEN=ghp_...
# GITHUB_REPO=ì‚¬ìš©ì/ë¦¬í¬
# GITHUB_PATH=news_data.json
# STRICT_TECH_KEYWORDS=1         # ê³µí•™Â·ê¸°ìˆ  í‚¤ì›Œë“œë§Œ í—ˆìš©
# SKIP_NON_TECH=0                # ë¹„ê¸°ìˆ  ê¸°ì‚¬ ìŠ¤í‚µ
# HIDE_NON_TECH_AT_UI=0          # UIì—ì„œ ë¹„ê¸°ìˆ  ê¸°ì‚¬ ìˆ¨ê¹€
# DB_PATH="C:/Users/ê¶Œë¯¼ì„œ/OneDrive/ë°”íƒ• í™”ë©´/app/news.db"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from __future__ import annotations
import os, sys, json, sqlite3, time
from typing import List, Dict, Tuple, Optional, Iterable, Set
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
import re
from translate_util import translate_rows_if_needed

import numpy as np
import pandas as pd
import requests
import feedparser
from bs4 import BeautifulSoup
from dotenv import load_dotenv

import streamlit as st
import streamlit.components.v1 as components

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from pyvis.network import Network

from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

st.set_page_config(page_title="ë‰´ìŠ¤ìˆìŠˆ~(News IT's Issue)", layout="wide")

# ============================================================
# 0) í™˜ê²½ì„¤ì •/í† ê¸€ ë¡œë“œ
# ============================================================
env_path = Path(__file__).resolve().with_name(".env")
load_dotenv(dotenv_path=env_path)

# ===== ì•ˆì „ íŒŒì„œ ìœ í‹¸ (ì£¼ì„/ë”°ì˜´í‘œ/ê³µë°± í—ˆìš©) =====
import re

def _strip_comment(v: str) -> str:
    v = (v or "").split('#', 1)[0].strip()
    if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
        v = v[1:-1].strip()
    return v

def getenv_int(name: str, default: int) -> int:
    raw = _strip_comment(os.getenv(name, str(default)))
    m = re.search(r'-?\d+', raw)
    return int(m.group()) if m else int(default)

def getenv_float(name: str, default: float) -> float:
    raw = _strip_comment(os.getenv(name, str(default)))
    m = re.search(r'-?\d+(?:\.\d+)?', raw)
    return float(m.group()) if m else float(default)

def getenv_bool(name: str, default: bool=False) -> bool:
    raw = _strip_comment(os.getenv(name, str(default))).lower()
    return raw in ("1", "true", "t", "yes", "y", "on")

def getenv_str(name: str, default: str="") -> str:
    return _strip_comment(os.getenv(name, default))

# ===== ì—¬ê¸°ë¶€í„° êµì²´ =====
MAX_RESULTS            = getenv_int("MAX_RESULTS", 10)
MAX_TOTAL_PER_SOURCE   = getenv_int("MAX_TOTAL_PER_SOURCE", 200)
RSS_BACKFILL_PAGES     = getenv_int("RSS_BACKFILL_PAGES", 3)

CONNECT_TIMEOUT        = getenv_float("CONNECT_TIMEOUT", 6.0)
READ_TIMEOUT           = getenv_float("READ_TIMEOUT", 10.0)
OPENAI_TIMEOUT         = getenv_float("OPENAI_TIMEOUT", 20.0)

ENABLE_SUMMARY         = getenv_bool("ENABLE_SUMMARY", False)
ENABLE_GITHUB          = getenv_bool("ENABLE_GITHUB", False)
ENABLE_HTTP_CACHE      = getenv_bool("ENABLE_HTTP_CACHE", True)
HTTP_CACHE_EXPIRE      = getenv_int("HTTP_CACHE_EXPIRE", 3600)
PARALLEL_MAX_WORKERS   = getenv_int("PARALLEL_MAX_WORKERS", 8)
SKIP_UPDATE_IF_EXISTS  = getenv_bool("SKIP_UPDATE_IF_EXISTS", True)

UI_LOAD_LIMIT          = getenv_int("UI_LOAD_LIMIT", 2000)

NLP_BACKEND            = getenv_str("NLP_BACKEND", "kiwi").lower()
PER_ARTICLE_SLEEP      = getenv_float("PER_ARTICLE_SLEEP", 0.0)

STRICT_TECH_KEYWORDS   = getenv_bool("STRICT_TECH_KEYWORDS", True)
SKIP_NON_TECH          = getenv_bool("SKIP_NON_TECH", False)
HIDE_NON_TECH_AT_UI    = getenv_bool("HIDE_NON_TECH_AT_UI", False)

OPENAI_API_KEY         = getenv_str("OPENAI_API_KEY", "ssu1")
GITHUB_TOKEN           = getenv_str("GITHUB_TOKEN",   "ssu2")
GITHUB_REPO            = getenv_str("GITHUB_REPO",    "ssu3")
GITHUB_PATH            = getenv_str("GITHUB_PATH",    "news_data.json")
DB_PATH                = getenv_str("DB_PATH", "news.db")
# ===== êµì²´ ë =====


def _require(name, value, placeholder):
    if value is None or value.strip() == "" or value.strip() == placeholder:
        raise RuntimeError(f"{name}ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. .envì—ì„œ {name} ê°’ì„ ì„¤ì •í•˜ì„¸ìš”.")

_openai_client = None
_github_client = None
def _get_openai():
    global _openai_client
    if _openai_client is None and ENABLE_SUMMARY:
        from openai import OpenAI
        _require("OPENAI_API_KEY", OPENAI_API_KEY, "ssu1")
        _openai_client = OpenAI(api_key=OPENAI_API_KEY, timeout=OPENAI_TIMEOUT)
    return _openai_client
def _get_github():
    global _github_client
    if _github_client is None and ENABLE_GITHUB:
        from github import Github
        _require("GITHUB_TOKEN", GITHUB_TOKEN, "ssu2")
        _require("GITHUB_REPO",  GITHUB_REPO,  "ssu3")
        _require("GITHUB_PATH",  GITHUB_PATH,  "ssu4")
        _github_client = Github(GITHUB_TOKEN)
    return _github_client

# ============================================================
# 1) DB ì¤€ë¹„ (+ ì¦ê²¨ì°¾ê¸°)
# ============================================================
# ============================================================
# 1) DB ì¤€ë¹„ (+ ì¦ê²¨ì°¾ê¸°)
# ============================================================
# ê¸°ì¡´: DB_PATH = os.getenv("DB_PATH", "news.db")ê°€ ìœ„ìª½ì— ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆìŒ
# ê²½ë¡œ ì •ë¦¬ + í´ë” ìë™ ìƒì„± (ë”°ì˜´í‘œ/í˜¼í•© ìŠ¬ë˜ì‹œ ì´ìŠˆ ì˜ˆë°©)
DB_PATH = os.path.expanduser(DB_PATH).strip().strip('"').strip("'")
DB_PATH = DB_PATH.replace("\\", "/")
db_dir = os.path.dirname(DB_PATH) or "."
os.makedirs(db_dir, exist_ok=True)

try:
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
except sqlite3.OperationalError as e:
    st.error(f"DB ì—´ê¸° ì‹¤íŒ¨\nê²½ë¡œ: {DB_PATH}\nì—ëŸ¬: {e}")
    st.stop()

cursor = conn.cursor()
cursor.execute("""
CREATE TABLE IF NOT EXISTS articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT,
    link TEXT UNIQUE,
    published TEXT,
    source TEXT,
    raw_text TEXT,
    summary TEXT,
    keywords TEXT,
    category TEXT,
    created_at TEXT DEFAULT (datetime('now'))
);
""")

cursor.execute("""
CREATE TABLE IF NOT EXISTS favorites (
    article_id INTEGER UNIQUE,
    created_at TEXT DEFAULT (datetime('now'))
);
""")
conn.commit()

# ============================================================
# 2) RSS ì†ŒìŠ¤ (í™•ì¥)
# ============================================================
FEEDS: List[Dict[str, str]] = [
    # --- Korea (ko) ---
    {"feed_url": "https://it.donga.com/feeds/rss/",            "source": "ITë™ì•„",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section902.xml",      "source": "ì „ìì‹ ë¬¸_ì†ë³´",         "category": "IT",           "lang": "ko"},
    {"feed_url": "https://rss.etnews.com/Section901.xml",      "source": "ì „ìì‹ ë¬¸_ì˜¤ëŠ˜ì˜ë‰´ìŠ¤",     "category": "IT",           "lang": "ko"},
    {"feed_url": "https://zdnet.co.kr/news/news_xml.asp",      "source": "ZDNet Korea",         "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.itworld.co.kr/rss/all.xml",      "source": "ITWorld Korea",       "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.ciokorea.com/rss/all.xml",       "source": "CIO Korea",           "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.bloter.net/feed",                "source": "Bloter",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://byline.network/feed/",               "source": "Byline Network",      "category": "IT",           "lang": "ko"},
    {"feed_url": "https://platum.kr/feed",                     "source": "Platum",              "category": "Startup",      "lang": "ko"},
    {"feed_url": "https://www.boannews.com/media/news_rss.xml","source": "ë³´ì•ˆë‰´ìŠ¤",             "category": "Security",     "lang": "ko"},
    {"feed_url": "https://it.chosun.com/rss.xml",              "source": "ITì¡°ì„ ",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.ddaily.co.kr/news_rss.php",      "source": "ë””ì§€í„¸ë°ì¼ë¦¬",           "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.kbench.com/rss.xml",             "source": "KBench",              "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.sedaily.com/rss/IT.xml",         "source": "ì„œìš¸ê²½ì œ IT",           "category": "IT",           "lang": "ko"},
    {"feed_url": "https://www.hankyung.com/feed/it",           "source": "í•œêµ­ê²½ì œ IT",            "category": "IT",           "lang": "ko"},

    # --- Global (en) ---
    {"feed_url": "https://techcrunch.com/feed/",               "source": "TechCrunch",          "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.eetimes.com/feed/",              "source": "EE Times",            "category": "Electronics",  "lang": "en"},
    {"feed_url": "https://spectrum.ieee.org/rss/fulltext",     "source": "IEEE Spectrum",       "category": "Engineering",  "lang": "en"},
    {"feed_url": "http://export.arxiv.org/rss/cs",             "source": "arXiv CS",            "category": "Research",     "lang": "en"},
    {"feed_url": "https://www.nature.com/nel/atom.xml",        "source": "Nature Electronics",  "category": "Research",     "lang": "en"},
    {"feed_url": "https://www.technologyreview.com/feed/",     "source": "MIT Tech Review",     "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.theverge.com/rss/index.xml",     "source": "The Verge",           "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.wired.com/feed/rss",             "source": "WIRED",               "category": "Tech",         "lang": "en"},
    {"feed_url": "https://www.engadget.com/rss.xml",           "source": "Engadget",            "category": "Tech",         "lang": "en"},
    {"feed_url": "https://venturebeat.com/category/ai/feed/",  "source": "VentureBeat AI",      "category": "AI",           "lang": "en"},
]

HEADERS = {"User-Agent": "Mozilla/5.0 (NewsAgent/1.0)"}
try:
    if os.getenv("ENABLE_HTTP_CACHE", "1") == "1":
        from requests_cache import CachedSession
        SESSION = CachedSession('http_cache', expire_after=int(os.getenv("HTTP_CACHE_EXPIRE","3600")))
    else:
        SESSION = requests.Session()
except Exception:
    SESSION = requests.Session()
ADAPTER = requests.adapters.HTTPAdapter(pool_connections=10, pool_maxsize=20, max_retries=1)
SESSION.mount("http://", ADAPTER)
SESSION.mount("https://", ADAPTER)

def canonicalize_link(url: str) -> str:
    try:
        u = urlparse(url)
        scheme = (u.scheme or "https").lower()
        netloc = (u.netloc or "").lower()
        path = (u.path or "").rstrip("/")
        drop = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content","utm_id","utm_name",
                "gclid","fbclid","igshid","spm","ref","ref_src","cmpid"}
        qs = [(k, v) for k, v in parse_qsl(u.query, keep_blank_values=True) if k.lower() not in drop]
        query = urlencode(qs, doseq=True)
        return urlunparse((scheme, netloc, path, u.params, query, ""))
    except Exception:
        return url

def extract_main_text(url: str) -> str:
    try:
        r = SESSION.get(url, headers=HEADERS, timeout=(CONNECT_TIMEOUT, READ_TIMEOUT))
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        art = soup.find("article")
        if art and art.get_text(strip=True):
            return art.get_text("\n", strip=True)
        candidates = soup.select(
            "div[id*='content'], div[class*='content'], "
            "div[id*='article'], div[class*='article'], "
            "section[id*='content'], section[class*='content'], "
            "div[id*='news'], div[class*='news']"
        )
        best = max((c for c in candidates), key=lambda c: len(c.get_text(strip=True)), default=None)
        if best and len(best.get_text(strip=True)) > 200:
            return best.get_text("\n", strip=True)
        md = soup.find("meta", attrs={"name":"description"}) or soup.find("meta", attrs={"property":"og:description"})
        if md and md.get("content"):
            return md["content"]
    except Exception:
        pass
    return ""

def parse_feed(feed_url: str):
    try:
        feed = feedparser.parse(feed_url)
        if not hasattr(feed, "entries"): return None
        return feed
    except Exception:
        return None

def expand_paged_feed_urls(feed_url: str, pages: int) -> List[str]:
    """WordPress ê³„ì—´ /feed/ ì¸ ê²½ìš° ?paged=2..N ë¡œ í™•ì¥í•´ ê³¼ê±° ê¸°ì‚¬ë„ ìˆ˜ì§‘"""
    urls = [feed_url]
    if re.search(r"/feed/?$", feed_url, re.IGNORECASE):
        for i in range(2, max(2, pages+1)):
            sep = "&" if "?" in feed_url else "?"
            urls.append(f"{feed_url}{sep}paged={i}")
    return urls

# ============================================================
# 3) í‚¤ì›Œë“œ í•„í„°/ìš”ì•½/í˜•íƒœì†Œ (ìš”ì•½ ì •í™” + ì‹¤íŒ¨ íœ´ë¦¬ìŠ¤í‹±)
# ============================================================
STOP_EXACT: Set[str] = set(map(str.lower, """
ìˆ ìˆ˜ ê¹€ ê¸¸ ê°€ ë§ d ì–¼ b ë°± ë³´ ìœ„ ë…„ ëª… ë°”ê¾¸ ë§Œ ê²ƒ jtbc x í•˜ê¸° ì‘ ë” ëŠ” ì€ ì´ ê°€ ë¥¼ ì— ì™€ ê³¼ ë„ ìœ¼ë¡œ ë¡œ ë¶€í„° ì—ì„œ ê¹Œì§€ ì—ê²Œ í•œ ì™€/ê³¼ ì—ê²Œì„œ í•˜ í•˜ë‹¤ ì…ë‹ˆë‹¤ ê¸°ì ì‚¬ì§„ ì œê³µ ì˜ìƒ ê¸°ì‚¬ ì…ë ¥ ì „ ë‚  ì£¼ ì›” ë…„ ì˜¤ëŠ˜ ë‚´ì¼ ì–´ì œ
""".split()))
STOP_WORDS = set([
    "ê¸°ì","ë‰´ìŠ¤","íŠ¹íŒŒì›","ì˜¤ëŠ˜","ë§¤ìš°","ê¸°ì‚¬","ì‚¬ì§„","ì˜ìƒ","ì œê³µ","ì…ë ¥",
    "ê²ƒ","ìˆ˜","ë“±","ë°","ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ì§€ë‚œ","ì´ë²ˆ","ê´€ë ¨","ëŒ€í•œ","í†µí•´","ëŒ€í•´","ìœ„í•´",
    "ì…ë‹ˆë‹¤","í•œë‹¤","í–ˆë‹¤","í•˜ì˜€ë‹¤","ì—ì„œëŠ”","ì—ì„œ","ëŒ€í•œ","ì´ë‚ ","ë¼ë©°","ë‹¤ê³ ","ì˜€ë‹¤","í–ˆë‹¤ê°€","í•˜ë©°",
]) | STOP_EXACT

TECH_ALLOW_TERMS = set(map(str.lower, """
ai ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹ ìƒì„±í˜• ì±—gpt ë¡œë³´í‹±ìŠ¤ ë¡œë´‡ ìë™í™” í˜‘ë™ë¡œë´‡
ë°˜ë„ì²´ ë©”ëª¨ë¦¬ dram nand ddr sram hbm ì‹œìŠ¤í…œ ë°˜ë„ì²´ íŒŒìš´ë“œë¦¬ ì›¨ì´í¼ ì†Œì ê³µì • ë…¸ê´‘ euv ì¥ë¹„ ì†Œì¬
npu tpu gpu cpu dsp isp fpga asic ì¹©ì…‹ ì¹© ì„¤ê³„ ë¦¬ì†Œê·¸ë˜í”¼ íŒ¨í‚¤ì§• í•˜ì´ë¸Œë¦¬ë“œ ë³¸ë”©
ì´ì°¨ì „ì§€ ë°°í„°ë¦¬ ess ì–‘ê·¹ì¬ ìŒê·¹ì¬ ì „í•´ì§ˆ ë¶„ë¦¬ë§‰ ê³ ì²´ì „ì§€ ì „ê³ ì²´ ì „ê¸°ì°¨ ev hev phev bms
ììœ¨ì£¼í–‰ ë¼ì´ë‹¤ ë ˆì´ë” ì„¼ì„œ ì¹´ë©”ë¼ ì œì–´ê¸° ecu v2x
í†µì‹  ë„¤íŠ¸ì›Œí¬ 5g 6g lte nr ìœ„ì„± mmwave ë°±í™€ fronthaul ìŠ¤ëª°ì…€
ict í´ë¼ìš°ë“œ ì—£ì§€ì»´í“¨íŒ… ì—£ì§€ ì»´í“¨íŒ… ì„œë²„ ë°ì´í„°ì„¼í„° ì¿ ë²„ë„¤í‹°ìŠ¤ ì»¨í…Œì´ë„ˆ devops cicd ì˜¤ë¸Œì íŠ¸ìŠ¤í† ë¦¬ì§€ ê°ì²´ì €ì¥
ì†Œí”„íŠ¸ì›¨ì–´ í”Œë«í¼ saas paas iaas ë³´ì•ˆ ì•”í˜¸ ì¸ì¦ í‚¤ê´€ë¦¬ í‚¤ì²´ì¸ ì·¨ì•½ì  ì œë¡œíŠ¸ëŸ¬ìŠ¤íŠ¸
í•€í…Œí¬ ë¸”ë¡ì²´ì¸ ë¶„ì‚°ì›ì¥ defi nft
ëª¨ë¸ í•™ìŠµ íŒŒì¸íŠœë‹ íŠœë‹ í”„ë¡¬í”„íŠ¸ ì¶”ë¡  ì¸í¼ëŸ°ìŠ¤ í† í° ì„ë² ë”© ê²½ëŸ‰í™” ì–‘ìí™” distillation ì§€ì‹ì¦ë¥˜
ì‚¬ë¬¼ì¸í„°ë„· iot ì‚°ì—…ìš©iot iiot plc scada mes erp
ë””ìŠ¤í”Œë ˆì´ oled qd ë§ˆì´í¬ë¡œ led lcd microled micro-led
ë°”ì´ì˜¤ ë°”ì´ì˜¤ì„¼ì„œ ìœ ì „ìì¹˜ë£Œì œ ì„¸í¬ì¹˜ë£Œì œ ì˜ë£Œê¸°ê¸° í—¬ìŠ¤ì¼€ì–´ ë””ì§€í„¸ í—¬ìŠ¤ ì›¨ì–´ëŸ¬ë¸” ì›ê²©ì§„ë£Œ
""".split()))
TECH_ALLOW_PATTERNS = [
    r"^llm$", r"^rlhf$", r"^rag$", r"^ssl$", r"^tls$", r"^ssh$", r"^api$", r"^sdk$",
    r"^[cg]pu$", r"^cpu$", r"^npu$", r"^tpu$", r"^fpga$", r"^asic$",
    r"^(5g|6g|4g|lte|nr)$",
    r"^dr?am$", r"^nand$", r"^hbm$",
    r"^(ai|ml|dl|nlp|cv)$",
    r"^ar|^vr|^xr$",
    r"^[a-z]+net$",
    r".*net$",
    r".*transformer.*",
    r".*diffusion.*",
    r".*foundation model.*",
]
TECH_ALLOW_REGEX = [re.compile(p, re.IGNORECASE) for p in TECH_ALLOW_PATTERNS]

def is_meaningless_token(w: str) -> bool:
    if not w: return True
    s = w.strip()
    if not s: return True
    sl = s.lower()
    if len(sl) == 1: return True
    if re.fullmatch(r"[\W_]+", sl): return True
    if re.fullmatch(r"\d+", sl): return True
    if re.fullmatch(r"[\u1100-\u11FF\u3130-\u318F]", s): return True
    if sl in STOP_EXACT: return True
    return False

def is_tech_term(w: str) -> bool:
    if not w: return False
    s = w.strip(); sl = s.lower()
    if sl in TECH_ALLOW_TERMS: return True
    for rx in TECH_ALLOW_REGEX:
        if rx.search(s): return True
    if re.fullmatch(r"[a-z]{2,}\d{1,2}", sl): return True
    if "ë°˜ë„ì²´" in s or "ììœ¨ì£¼í–‰" in s or "í´ë¼ìš°ë“œ" in s or "ëª¨ë¸" in s or "ì•Œê³ ë¦¬ì¦˜" in s:
        return True
    return False

def sanitize_summary(s: Optional[str]) -> str:
    if not s: return ""
    t = str(s).strip()
    t = re.sub(r"^\s*\[[^\]]*\]\s*", "", t)
    t = re.sub(r"(^|\s)ì œëª©\s*:\s*", r"\1", t)
    t = re.sub(r"(^|\s)ì²«\s*ë¬¸ì¥\s*:\s*", r"\1", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _clean_sentences(txt: str, limit_chars: int = 700) -> str:
    t = (txt or "").strip()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"(ê¸°ì|ì‚¬ì§„|ì˜ìƒ)\s*=", "", t)
    return t[:limit_chars]

# ---- ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì œëª© ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ìš”ì•½ ----
def generate_heuristic_summary(title: str, source: str, published: str,
                               categories_map: Optional[Dict[str, Dict[str, List[str]]]] = None) -> str:
    title_str = (title or "").strip()
    pm, ps = "IT/ê³µí•™", "í•µì‹¬ ì´ìŠˆ"
    if categories_map:
        # ì œëª©ì— í¬í•¨ëœ í‚¤ì›Œë“œë¡œ ëŒ€ê°• ë¶„ë¥˜
        for main, subcats in categories_map.items():
            for sub, kws in subcats.items():
                for kw in kws:
                    if kw and str(kw) in title_str:
                        pm, ps = main, sub
                        break
    # ê°„ë‹¨í•œ ì„¤ëª… 2~3ë¬¸ì¥ + í•´ì‹œíƒœê·¸
    base = f"{source} ë³´ë„. '{title_str}' ì£¼ì œì˜ {pm} - {ps} ê´€ë ¨ ì´ìŠˆì…ë‹ˆë‹¤. " \
           f"í•´ë‹¹ ë¶„ì•¼ëŠ” ìµœê·¼ ê¸°ìˆ Â·ì œí’ˆÂ·íˆ¬ì ë™í–¥ì´ í™œë°œí•˜ë©° ì‚°ì—… ì „ë°˜ì˜ ê²½ìŸì´ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. " \
           f"ê¸°ì—…/ì—°êµ¬ê¸°ê´€ì˜ ë°œí‘œì™€ í‘œì¤€í™”, ìƒíƒœê³„ í™•ì¥ì´ ë™ì‹œì— ì§„í–‰ë˜ëŠ” íë¦„ì„ ì°¸ê³ í•˜ì„¸ìš”."
    # í•´ì‹œíƒœê·¸(ì œëª©ì—ì„œ ë½‘ê¸°)
    tags = []
    for token in re.split(r"[^\wê°€-í£A-Za-z\+\-]+", title_str):
        if not token: continue
        if is_meaningless_token(token): continue
        if is_tech_term(token):
            tags.append(token)
    tags = list(dict.fromkeys(tags))[:4]
    if tags:
        base += "  #" + " #".join(tags)
    return sanitize_summary(base)

def summarize_kor(title: str, source: str, published: str, text: str) -> str:
    def _fallback():
        base = _clean_sentences(text or "", 1200)
        if not base or len(base) < 20:
            return sanitize_summary(f"{title}".strip())
        sents = re.split(r"(?<=[.?!ã€‚])\s+", base)
        head = " ".join(sents[:3]) if sents else base
        return sanitize_summary(head)

    if not ENABLE_SUMMARY:
        # ëª¨ë¸ì„ ì“°ì§€ ì•ŠëŠ” ëª¨ë“œì—ì„œëŠ” íœ´ë¦¬ìŠ¤í‹± ë³´ê°•
        out = _fallback()
        if not out or out.strip().lower() == (title or "").strip().lower() or len(out) < 60:
            try:
                cm = globals().get("CATEGORIES", None)
                return generate_heuristic_summary(title, source, published, categories_map=cm)
            except Exception:
                return out or (title or "")
        return out

    try:
        client = _get_openai()
        snippet = _clean_sentences(text or "", 6000)
        prompt = f"""
ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ì‚¬ì‹¤ ìœ„ì£¼ë¡œ 4~6ë¬¸ì¥ í•œêµ­ì–´ ìš”ì•½í•˜ì„¸ìš”. ê³¼ì¥/ì˜ê²¬ ì—†ì´ í•µì‹¬ë§Œ.
- ì œëª©: {title}
- ë§¤ì²´: {source}
- ê²Œì‹œì¼: {published}
- ë³¸ë¬¸(ë°œì·Œ): {snippet}
ìš”êµ¬ì‚¬í•­:
1) í•µì‹¬ ê¸°ìˆ /ì œí’ˆ/ì¡°ì¹˜/ìˆ˜ì¹˜/ì¼ì •
2) ì‚°ì—…ì  í•¨ì˜ 1ì¤„
3) ë§ˆì§€ë§‰ì— #í‚¤ì›Œë“œ 3~5ê°œ (ì‰¼í‘œ êµ¬ë¶„, ê¸°ìˆ  ê´€ë ¨)
"""
        chat = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role":"system","content":"ë„ˆëŠ” ê³µí•™Â·IT ë‰´ìŠ¤ í•œêµ­ì–´ ì—ë””í„°ë‹¤. ì‚¬ì‹¤ë§Œ ê°„ê²°íˆ."},
                {"role":"user","content":prompt}
            ],
            temperature=0.3, max_tokens=420, timeout=OPENAI_TIMEOUT,
        )
        out = (chat.choices[0].message.content or "").strip()
        out = sanitize_summary(out)
        if not out or out.strip().lower() == (title or "").strip().lower() or len(out) < 60:
            cm = globals().get("CATEGORIES", None)
            return generate_heuristic_summary(title, source, published, categories_map=cm)
        return out
    except Exception:
        # OpenAI ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±
        cm = globals().get("CATEGORIES", None)
        return generate_heuristic_summary(title, source, published, categories_map=cm)

# í˜•íƒœì†Œ
pos_tags = None
token_filter = None
def _init_nlp():
    global pos_tags, token_filter
    if pos_tags is not None: return
    try:
        if NLP_BACKEND in ("auto", "okt"):
            from konlpy.tag import Okt
            _okt = Okt()
            def _pos(txt): return _okt.pos(txt or "", norm=True, stem=True)
            def _filter(w,p): return p in {"Noun","Verb","Adjective","Alpha"}
            pos_tags, token_filter = _pos, _filter
            return
        raise RuntimeError("Okt ë¯¸ì‚¬ìš© ê°•ì œ")
    except Exception:
        if NLP_BACKEND == "okt":
            raise
        from kiwipiepy import Kiwi
        _kiwi = Kiwi()
        def _pos(txt):
            txt = txt or ""
            try: toks = _kiwi.tokenize(txt)
            except Exception:
                res = _kiwi.analyze(txt) or []
                r = res[0] if res else None
                toks = getattr(r, "tokens", None)
                if toks is None and isinstance(r, (tuple, list)) and len(r) >= 2:
                    toks = r[1]
                toks = toks or []
            pairs = []
            for t in toks:
                if hasattr(t, "form"): form, tag = t.form, str(t.tag)
                elif isinstance(t, (tuple, list)) and len(t) >= 2: form, tag = t[0], str(t[1])
                else: continue
                if   tag.startswith("NN"): mapped = "Noun"
                elif tag.startswith("VV"): mapped = "Verb"
                elif tag.startswith("VA"): mapped = "Adjective"
                elif tag == "SL":         mapped = "Alpha"
                else:                     mapped = tag
                pairs.append((form, mapped))
            return pairs
        def _filter(w,p): return p in {"Noun","Verb","Adjective","Alpha"}
        pos_tags, token_filter = _pos, _filter
_init_nlp()

def extract_keywords(text: str, top_k: int = 30):
    _init_nlp()
    if not text: return []
    toks = []
    for w, p in pos_tags(text):
        if p not in {"Noun","Verb","Adjective","Alpha"}: continue
        wl = str(w).strip()
        if not wl: continue
        if wl.lower() in STOP_WORDS: continue
        if is_meaningless_token(wl): continue
        toks.append(wl)
    if STRICT_TECH_KEYWORDS:
        toks = [t for t in toks if is_tech_term(t)]
    counts = Counter(toks)
    if not counts:
        for w,p in pos_tags(text):
            if p != "Noun": continue
            w2 = str(w).strip()
            if len(w2) >= 2 and not is_meaningless_token(w2):
                counts[w2] += 1
    return [w for w,_ in counts.most_common(top_k)]

def is_tech_doc(title: str, body: str, keywords: Iterable[str]) -> bool:
    text = f"{title or ''} {body or ''} {' '.join(keywords or [])}"
    for k in (keywords or []):
        if is_tech_term(k): return True
    for term in TECH_ALLOW_TERMS:
        if term in text.lower(): return True
    for rx in TECH_ALLOW_REGEX:
        if rx.search(text): return True
    if re.search(r"(ì—°ì˜ˆ|ìŠ¤íƒ€|ì˜ˆëŠ¥|í—¬ìŠ¤|ê±´ê°•|ë¼ì´í”„|ë§›ì§‘|ì—¬í–‰|ë·°í‹°|ìš´ì„¸|ê²Œì„ì‡¼|eìŠ¤í¬ì¸ )", text):
        return False
    return False

# ============================================================
# 4) ì—…ì„œíŠ¸/ìˆ˜ì§‘ (ë°±í•„ í¬í•¨)
# ============================================================
def link_exists(link: str) -> bool:
    try:
        cursor.execute("SELECT 1 FROM articles WHERE link=? LIMIT 1", (link,))
        return cursor.fetchone() is not None
    except Exception:
        return False

def upsert_article(title, link, published, source, raw_text, summary, keywords):
    link = canonicalize_link(link)
    summary = sanitize_summary(summary)
    try:
        cursor.execute("""
            INSERT INTO articles (title, link, published, source, raw_text, summary, keywords, category)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (title, link, published, source, raw_text, summary,
              json.dumps(keywords, ensure_ascii=False), None))
        conn.commit()
        return "insert"
    except sqlite3.IntegrityError:
        cursor.execute("""
            UPDATE articles
               SET title=?, published=?, source=?,
                   raw_text=?, summary=?, keywords=?
             WHERE link=?
        """, (title, published, source, raw_text, summary,
              json.dumps(keywords, ensure_ascii=False), link))
        conn.commit()
        return "update"

def _process_entry(entry, idx, total, source, log):
    title = getattr(entry, "title", "").strip()
    link  = canonicalize_link(getattr(entry, "link", "").strip())
    if not (title and link):
        return "skip", idx

    if SKIP_UPDATE_IF_EXISTS and link_exists(link):
        return "skip", idx

    published = getattr(entry, "published", "") or getattr(entry, "updated", "") or datetime.utcnow().strftime("%Y-%m-%d")
    raw_text  = extract_main_text(link) or getattr(entry, "summary", "") or ""
    summary   = summarize_kor(title, source, published, raw_text or title)
    keywords  = extract_keywords(raw_text or summary, top_k=30)

    if SKIP_NON_TECH and not is_tech_doc(title, raw_text, keywords):
        return "skip_nontech", idx

    res = upsert_article(title, link, published, source, raw_text, summary, keywords)

    if PER_ARTICLE_SLEEP > 0:
        time.sleep(PER_ARTICLE_SLEEP)
    return res, idx

def fetch_and_store_news(feed_url: str, source: str, max_total=None, log=None):
    if max_total is None:
        max_total = MAX_TOTAL_PER_SOURCE
    if log is None:
        log = st.write

    if not feed_url:
        log(f"- {source}: feed_url ì—†ìŒ â†’ ê±´ë„ˆëœ€")
        return

    urls = expand_paged_feed_urls(feed_url, RSS_BACKFILL_PAGES)
    log(f"**â–· {source}** í”¼ë“œ ì½ëŠ” ì¤‘â€¦ (í™•ì¥ {len(urls)}ê°œ)")

    entries_all = []
    for i, u in enumerate(urls, 1):
        feed = parse_feed(u)
        if feed is None:
            log(f"  - RSS íŒŒì‹± ì‹¤íŒ¨/ë¹„í˜¸í™˜: {u} â†’ ê±´ë„ˆëœ€")
            continue
        entries = feed.entries or []
        if MAX_RESULTS:
            entries = entries[:MAX_RESULTS]
        entries_all.extend(entries)
        log(f"  - {i}/{len(urls)}: í•­ëª© {len(entries)}ê±´")

        if len(entries_all) >= max_total:
            break

    if not entries_all:
        log(f"â—¼ {source}: ìˆ˜ì§‘ëœ í•­ëª© ì—†ìŒ")
        return

    # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
    seen = set()
    uniq_entries = []
    for e in entries_all:
        link = canonicalize_link(getattr(e, "link", ""))
        if not link or link in seen:
            continue
        seen.add(link)
        uniq_entries.append(e)

    log(f"- ì´ ì²˜ë¦¬ ëŒ€ìƒ: {len(uniq_entries)}ê±´ (ì¤‘ë³µ ì œê±° í›„)")

    inserted, updated, skipped, skipped_nontech = 0, 0, 0, 0

    if not ENABLE_SUMMARY and len(uniq_entries) > 0:
        workers = max(1, PARALLEL_MAX_WORKERS)
        with ThreadPoolExecutor(max_workers=workers) as ex:
            futs = [ex.submit(_process_entry, e, i, len(uniq_entries), source, log)
                    for i, e in enumerate(uniq_entries, 1)]
            for fut in as_completed(futs):
                res, idx = fut.result()
                if   res == "insert": inserted += 1; log(f"[{idx}/{len(uniq_entries)}] âœ… ì‹ ê·œ ì €ì¥")
                elif res == "update": updated  += 1; log(f"[{idx}/{len(uniq_entries)}] ğŸ”„ ì—…ë°ì´íŠ¸")
                elif res == "skip_nontech":    skipped_nontech += 1
                else:                           skipped += 1
    else:
        for idx, entry in enumerate(uniq_entries, 1):
            res, _ = _process_entry(entry, idx, len(uniq_entries), source, log)
            if   res == "insert": inserted += 1; log(f"[{idx}/{len(uniq_entries)}] âœ… ì‹ ê·œ ì €ì¥")
            elif res == "update": updated  += 1; log(f"[{idx}/{len(uniq_entries)}] ğŸ”„ ì—…ë°ì´íŠ¸")
            elif res == "skip_nontech":    skipped_nontech += 1
            else:                           skipped += 1

    log(f"â—¼ {source}: ì‹ ê·œ {inserted} Â· ì—…ë°ì´íŠ¸ {updated} Â· ìŠ¤í‚µ {skipped} Â· ë¹„ê¸°ìˆ ìŠ¤í‚µ {skipped_nontech}")

def ingest_all(log=None):
    if log is None:
        log = st.write
    log("â³ ë‰´ìŠ¤ ìˆ˜ì§‘/ìš”ì•½/í‚¤ì›Œë“œ ì‹œì‘")
    for f in FEEDS:
        fetch_and_store_news(f.get("feed_url"), f.get("source","(unknown)"), log=log)
    log("âœ… ëª¨ë“  ì†ŒìŠ¤ ì²˜ë¦¬ ì™„ë£Œ")

def export_json(path="news_data.json", limit=500, log=None):
    if log is None:
        log = st.write
    df = pd.read_sql("""
        SELECT title, link, published, source, summary, keywords
        FROM articles
        ORDER BY id DESC LIMIT ?
    """, conn, params=(limit,))
    df["summary"] = df["summary"].apply(sanitize_summary)
    records = df.to_dict(orient="records")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(records, f, ensure_ascii=False, indent=2)
    log(f"ğŸ’¾ JSON ì €ì¥: {path} ({len(records)}ê±´)")
    return path

def upload_to_github(local_path: str, repo_fullname: str, dest_path: str, log=None):
    if log is None:
        log = st.write
    if not ENABLE_GITHUB:
        log("â†ªï¸ ê¹ƒí—ˆë¸Œ ì—…ë¡œë“œ ë¹„í™œì„±í™”(ENABLE_GITHUB=0) â€” ê±´ë„ˆëœ€")
        return
    gh = _get_github()
    repo = gh.get_repo(repo_fullname)
    with open(local_path, "r", encoding="utf-8") as f:
        content = f.read()
    try:
        existing = repo.get_contents(dest_path)
        repo.update_file(dest_path, f"Update news ({datetime.now().strftime('%Y-%m-%d %H:%M')})", content, existing.sha)
        log(f"â¬†ï¸ GitHub ì—…ë°ì´íŠ¸ ì™„ë£Œ: {repo_fullname}/{dest_path}")
    except Exception:
        repo.create_file(dest_path, f"Add news ({datetime.now().strftime('%Y-%m-%d %H:%M')})", content)
        log(f"â¬†ï¸ GitHub ì‹ ê·œ ì—…ë¡œë“œ ì™„ë£Œ: {repo_fullname}/{dest_path}")

# ============================================================
# 5) ë¡œë”©/íƒ€ì„ì¡´/ì¦ê²¨ì°¾ê¸°
# ============================================================
@st.cache_data(show_spinner=False)
def load_df_from_db(limit: Optional[int] = UI_LOAD_LIMIT) -> pd.DataFrame:
    sql = """SELECT id, title, summary, keywords, source,
                    published AS published_at
             FROM articles
             ORDER BY id DESC"""
    if limit:
        sql += f" LIMIT {limit}"
    df = pd.read_sql(sql, conn)
    if df.empty:
        return df
    
    # keywords JSON íŒŒì‹±
    def parse_kw(kw_json):
        if not kw_json:
            return []
        try:
            return json.loads(kw_json)
        except (json.JSONDecodeError, TypeError):
            return []
    
    df["keywords"] = df["keywords"].apply(parse_kw)
    
    # published_atì„ datetime ë³€í™˜
    def safe_parse_date(date_str):
        if pd.isna(date_str) or not str(date_str).strip():
            return pd.NaT
        try:
            return pd.to_datetime(str(date_str).strip())
        except:
            return pd.NaT
    
    df["published_at"] = df["published_at"].apply(safe_parse_date)
    return df

def is_favorite(article_id: int) -> bool:
    cursor.execute("SELECT 1 FROM favorites WHERE article_id=? LIMIT 1", (article_id,))
    return cursor.fetchone() is not None

def toggle_favorite(article_id: int) -> bool:
    if is_favorite(article_id):
        cursor.execute("DELETE FROM favorites WHERE article_id=?", (article_id,))
        conn.commit()
        return False
    else:
        cursor.execute("INSERT OR IGNORE INTO favorites (article_id) VALUES (?)", (article_id,))
        conn.commit()
        return True

@st.cache_data(show_spinner=False)
def get_all_sources() -> List[str]:
    cursor.execute("SELECT DISTINCT source FROM articles ORDER BY source")
    return [r[0] for r in cursor.fetchall()]

# ============================================================
# 6) í‚¤ì›Œë“œ ì‹œê°í™” (ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„)
# ============================================================
def generate_keyword_network_graph(df: pd.DataFrame, top_n: int = 30) -> str:
    """í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ HTML ìƒì„±"""
    if df.empty:
        return "<p>ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
    all_keywords = []
    keyword_cooccurrence = defaultdict(int)
    
    for keywords_list in df["keywords"]:
        if keywords_list:
            keywords = [kw for kw in keywords_list if kw.strip()]
            all_keywords.extend(keywords)
            
            # ë™ì‹œ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
            for i, kw1 in enumerate(keywords):
                for kw2 in keywords[i+1:]:
                    pair = tuple(sorted([kw1, kw2]))
                    keyword_cooccurrence[pair] += 1
    
    # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
    keyword_counts = Counter(all_keywords)
    top_keywords = [kw for kw, _ in keyword_counts.most_common(top_n)]
    
    if not top_keywords:
        return "<p>í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
    
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
    net = Network(height="400px", width="100%", bgcolor="#ffffff", font_color="black")
    
    # ë…¸ë“œ ì¶”ê°€
    for kw in top_keywords:
        size = min(50, max(10, keyword_counts[kw] * 2))
        net.add_node(kw, label=kw, size=size)
    
    # ì—£ì§€ ì¶”ê°€ (ë™ì‹œ ì¶œí˜„ ë¹ˆë„ê°€ ë†’ì€ ê²ƒë§Œ)
    for (kw1, kw2), weight in keyword_cooccurrence.items():
        if kw1 in top_keywords and kw2 in top_keywords and weight > 1:
            net.add_edge(kw1, kw2, weight=weight)
    
    # HTML ë°˜í™˜
    try:
        return net.generate_html()
    except:
        return "<p>ë„¤íŠ¸ì›Œí¬ ìƒì„± ì‹¤íŒ¨</p>"

def create_wordcloud(df: pd.DataFrame) -> plt.Figure:
    """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if df.empty:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", ha='center', va='center', fontsize=20)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        return fig
    
    # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
    all_keywords = []
    for keywords_list in df["keywords"]:
        if keywords_list:
            all_keywords.extend([kw for kw in keywords_list if kw.strip()])
    
    if not all_keywords:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, "í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤", ha='center', va='center', fontsize=20)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        return fig
    
    # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
    keyword_freq = Counter(all_keywords)
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    try:
        from matplotlib import font_manager
        # í•œê¸€ í°íŠ¸ ì„¤ì • (ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        font_path = None
        for font_name in ["NanumGothic", "Malgun Gothic", "AppleGothic"]:
            fonts = font_manager.findSystemFonts(fontpaths=None, fontext='ttf')
            for font in fonts:
                if font_name.lower() in font.lower():
                    font_path = font
                    break
            if font_path:
                break
        
        wordcloud = WordCloud(
            font_path=font_path,
            width=800, height=400,
            background_color='white',
            colormap='viridis',
            max_words=100,
            relative_scaling=0.5,
            stopwords=STOPWORDS
        ).generate_from_frequencies(keyword_freq)
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        return fig
        
    except Exception as e:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}", ha='center', va='center', fontsize=16)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        return fig

# ============================================================
# 7) ë©”ì¸ UI
# ============================================================
def main():
    # ì‚¬ì´ë“œë°”: í•„í„°ë§ ì˜µì…˜
    with st.sidebar:
        st.header("ğŸ”§ í•„í„°ë§")
        
        # ì†ŒìŠ¤ í•„í„°
        available_sources = get_all_sources()
        selected_sources = st.multiselect(
            "ë‰´ìŠ¤ ì†ŒìŠ¤",
            options=available_sources,
            default=available_sources[:5] if len(available_sources) > 5 else available_sources
        )
        
        # í‚¤ì›Œë“œ ê²€ìƒ‰
        search_keyword = st.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰", placeholder="ì˜ˆ: AI, ë°˜ë„ì²´, 5G")
        
        # ê¸°ê°„ í•„í„°
        col1, col2 = st.columns(2)
        with col1:
            date_from = st.date_input("ì‹œì‘ì¼", value=datetime.now() - timedelta(days=7))
        with col2:
            date_to = st.date_input("ì¢…ë£Œì¼", value=datetime.now())
        
        # ì¦ê²¨ì°¾ê¸°ë§Œ ë³´ê¸°
        favorites_only = st.checkbox("ì¦ê²¨ì°¾ê¸°ë§Œ ë³´ê¸°")
        
        st.divider()
        
        # ë°ì´í„° ê´€ë¦¬
        st.header("ğŸ“Š ë°ì´í„° ê´€ë¦¬")
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ë²„íŠ¼
        if st.button("ğŸ”„ ë‰´ìŠ¤ ìˆ˜ì§‘", type="primary"):
            with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                ingest_all()
            st.success("ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ!")
            st.rerun()
        
        # JSON ë‚´ë³´ë‚´ê¸°
        if st.button("ğŸ’¾ JSON ë‚´ë³´ë‚´ê¸°"):
            export_path = export_json()
            st.success(f"JSON íŒŒì¼ ì €ì¥: {export_path}")
            
            # GitHub ì—…ë¡œë“œ (ì„¤ì •ëœ ê²½ìš°)
            if ENABLE_GITHUB:
                try:
                    upload_to_github(export_path, GITHUB_REPO, GITHUB_PATH)
                    st.success("GitHub ì—…ë¡œë“œ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"GitHub ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë©”ì¸ ì»¨í…ì¸ 
    st.title("ğŸ—ï¸ ë‰´ìŠ¤ìˆìŠˆ~(News IT's Issue)")
    st.markdown("**IT/ê³µí•™ ë‰´ìŠ¤ ìˆ˜ì§‘, ë¶„ì„, ì‹œê°í™” ëŒ€ì‹œë³´ë“œ**")
    
    # íƒ­ êµ¬ì„±
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“° ë‰´ìŠ¤ ëª©ë¡", "ğŸ“Š í‚¤ì›Œë“œ ë¶„ì„", "â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ", "â­ ì¦ê²¨ì°¾ê¸°"])
    
    # ë°ì´í„° ë¡œë“œ
    df = load_df_from_db()
    
    if df.empty:
        st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'ë‰´ìŠ¤ ìˆ˜ì§‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
        return
    
    # í•„í„°ë§ ì ìš©
    filtered_df = df.copy()
    
    # ì†ŒìŠ¤ í•„í„°
    if selected_sources:
        filtered_df = filtered_df[filtered_df["source"].isin(selected_sources)]
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰
    if search_keyword:
        keyword_mask = filtered_df["keywords"].apply(
            lambda kws: any(search_keyword.lower() in str(kw).lower() for kw in (kws or []))
        ) | filtered_df["title"].str.contains(search_keyword, case=False, na=False) | \
        filtered_df["summary"].str.contains(search_keyword, case=False, na=False)
        filtered_df = filtered_df[keyword_mask]
    
    # ê¸°ê°„ í•„í„°
    if date_from and date_to and not filtered_df.empty:
        # published_atì´ datetimeì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³€í™˜
        if filtered_df["published_at"].dtype == 'object':
            filtered_df["published_at"] = pd.to_datetime(filtered_df["published_at"], errors='coerce')
        
        # NaT ê°’ì´ ì•„ë‹Œ ê²ƒë§Œ í•„í„°ë§
        valid_dates = ~filtered_df["published_at"].isna()
        if valid_dates.any():
            date_mask = valid_dates & \
                       (filtered_df["published_at"].dt.date >= date_from) & \
                       (filtered_df["published_at"].dt.date <= date_to)
            filtered_df = filtered_df[date_mask]
    
    # ì¦ê²¨ì°¾ê¸° í•„í„°
    if favorites_only:
        favorite_ids = []
        cursor.execute("SELECT article_id FROM favorites")
        favorite_ids = [row[0] for row in cursor.fetchall()]
        if favorite_ids:
            filtered_df = filtered_df[filtered_df["id"].isin(favorite_ids)]
        else:
            filtered_df = pd.DataFrame()  # ì¦ê²¨ì°¾ê¸°ê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame
    
    # ë¹„ê¸°ìˆ  ê¸°ì‚¬ ìˆ¨ê¹€
    if HIDE_NON_TECH_AT_UI:
        tech_mask = filtered_df.apply(
            lambda row: is_tech_doc(row["title"], "", row["keywords"]), axis=1
        )
        filtered_df = filtered_df[tech_mask]
    
    with tab1:
        st.header("ğŸ“° ë‰´ìŠ¤ ëª©ë¡")
        st.markdown(f"**ì´ {len(filtered_df)}ê±´ì˜ ë‰´ìŠ¤**")
        
        if filtered_df.empty:
            st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # í˜ì´ì§€ë„¤ì´ì…˜
            items_per_page = 10
            total_pages = max(1, (len(filtered_df) + items_per_page - 1) // items_per_page)
            
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                page = st.selectbox("í˜ì´ì§€", range(1, total_pages + 1)) - 1
            
            start_idx = page * items_per_page
            end_idx = min(start_idx + items_per_page, len(filtered_df))
            page_df = filtered_df.iloc[start_idx:end_idx]
            
            # ë‰´ìŠ¤ ì¹´ë“œ í‘œì‹œ
            for idx, row in page_df.iterrows():
                with st.container():
                    col1, col2 = st.columns([10, 1])
                    
                    with col1:
                        # ì œëª©ê³¼ ê¸°ë³¸ ì •ë³´
                        st.markdown(f"### [{row['title']}]({row.get('link', '#')})")
                        
                        # ë©”íƒ€ë°ì´í„°
                        meta_col1, meta_col2, meta_col3 = st.columns(3)
                        with meta_col1:
                            st.markdown(f"**ğŸ“° {row['source']}**")
                        with meta_col2:
                            if pd.notna(row['published_at']):
                                try:
                                    if isinstance(row['published_at'], str):
                                        pub_date = pd.to_datetime(row['published_at'])
                                    else:
                                        pub_date = row['published_at']
                                    st.markdown(f"**ğŸ“… {pub_date.strftime('%Y-%m-%d %H:%M')}**")
                                except:
                                    st.markdown(f"**ğŸ“… {row['published_at']}**")
                        with meta_col3:
                            st.markdown(f"**ğŸ”— ID: {row['id']}**")
                        
                        # ìš”ì•½
                        if row["summary"]:
                            st.markdown(row["summary"])
                        
                        # í‚¤ì›Œë“œ
                        if row["keywords"]:
                            keywords_str = " Â· ".join([f"`{kw}`" for kw in row["keywords"][:10]])
                            st.markdown(f"**ğŸ·ï¸ í‚¤ì›Œë“œ:** {keywords_str}")
                    
                    with col2:
                        # ì¦ê²¨ì°¾ê¸° ë²„íŠ¼
                        is_fav = is_favorite(row["id"])
                        fav_label = "â­" if is_fav else "â˜†"
                        if st.button(fav_label, key=f"fav_{row['id']}"):
                            toggle_favorite(row["id"])
                            st.rerun()
                    
                    st.divider()
    
    with tab2:
        st.header("ğŸ“Š í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„")
        
        if filtered_df.empty:
            st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # í‚¤ì›Œë“œ í†µê³„
            all_keywords = []
            for keywords_list in filtered_df["keywords"]:
                if keywords_list:
                    all_keywords.extend([kw for kw in keywords_list if kw.strip()])
            
            if all_keywords:
                keyword_counts = Counter(all_keywords)
                
                col1, col2 = st.columns([1, 1])
                
                with col1:
                    st.subheader("ğŸ”¥ ì¸ê¸° í‚¤ì›Œë“œ TOP 20")
                    top_keywords = keyword_counts.most_common(20)
                    for i, (kw, count) in enumerate(top_keywords, 1):
                        st.markdown(f"{i}. **{kw}** ({count}íšŒ)")
                
                with col2:
                    st.subheader("ğŸ“ˆ í‚¤ì›Œë“œ ë¶„í¬")
                    top_kw_df = pd.DataFrame(top_keywords, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
                    st.bar_chart(top_kw_df.set_index("í‚¤ì›Œë“œ"))
                
                # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„
                st.subheader("ğŸ•¸ï¸ í‚¤ì›Œë“œ ê´€ê³„ ë„¤íŠ¸ì›Œí¬")
                network_html = generate_keyword_network_graph(filtered_df)
                components.html(network_html, height=450)
            else:
                st.info("í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.header("â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
        
        if filtered_df.empty:
            st.info("ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            wordcloud_fig = create_wordcloud(filtered_df)
            st.pyplot(wordcloud_fig)
    
    with tab4:
        st.header("â­ ì¦ê²¨ì°¾ê¸°")
        
        # ì¦ê²¨ì°¾ê¸° ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        favorite_sql = """
        SELECT a.id, a.title, a.summary, a.keywords, a.source, a.published as published_at
        FROM articles a
        INNER JOIN favorites f ON a.id = f.article_id
        ORDER BY f.created_at DESC
        """
        favorite_df = pd.read_sql(favorite_sql, conn)
        
        if favorite_df.empty:
            st.info("ì¦ê²¨ì°¾ê¸°í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # keywords JSON íŒŒì‹±
            def parse_kw(kw_json):
                if not kw_json:
                    return []
                try:
                    return json.loads(kw_json)
                except (json.JSONDecodeError, TypeError):
                    return []
            
            favorite_df["keywords"] = favorite_df["keywords"].apply(parse_kw)
            
            st.markdown(f"**ì´ {len(favorite_df)}ê±´ì˜ ì¦ê²¨ì°¾ê¸°**")
            
            # ì¦ê²¨ì°¾ê¸° ë‰´ìŠ¤ í‘œì‹œ
            for idx, row in favorite_df.iterrows():
                with st.container():
                    col1, col2 = st.columns([10, 1])
                    
                    with col1:
                        st.markdown(f"### {row['title']}")
                        
                        # ë©”íƒ€ë°ì´í„°
                        meta_col1, meta_col2 = st.columns(2)
                        with meta_col1:
                            st.markdown(f"**ğŸ“° {row['source']}**")
                        with meta_col2:
                            st.markdown(f"**ğŸ“… {row['published_at']}**")
                        
                        # ìš”ì•½
                        if row["summary"]:
                            st.markdown(row["summary"])
                        
                        # í‚¤ì›Œë“œ
                        if row["keywords"]:
                            keywords_str = " Â· ".join([f"`{kw}`" for kw in row["keywords"][:10]])
                            st.markdown(f"**ğŸ·ï¸ í‚¤ì›Œë“œ:** {keywords_str}")
                    
                    with col2:
                        # ì¦ê²¨ì°¾ê¸° ì œê±° ë²„íŠ¼
                        if st.button("ğŸ—‘ï¸", key=f"remove_fav_{row['id']}"):
                            toggle_favorite(row["id"])
                            st.rerun()
                    
                    st.divider()

if __name__ == "__main__":
    main()